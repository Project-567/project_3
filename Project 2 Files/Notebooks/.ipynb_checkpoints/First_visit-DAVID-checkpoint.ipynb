{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld with First Visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from random import uniform\n",
    "import random\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [[-1, 0], [0, 1], [1, 0], [0, -1]] #up, right, down, left = (clockwise from up) \n",
    "action_count = len(actions) # total number of actions\n",
    "gridSize = 5 # create a square grid of gridSize by gridSize\n",
    "state_count = gridSize*gridSize # total number of states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld():\n",
    "    def __init__(self, gridSize):\n",
    "        self.valueMap = np.zeros((gridSize, gridSize))\n",
    "        self.states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "        self.size = gridSize\n",
    "        self.new_pos = [0, 0] # initialize new position for p_transition\n",
    "        self.transition_prob = 1 # deterministic\n",
    "    \n",
    "    def initial_state(self):        # return initial state\n",
    "        return grid.states[gridSize*gridSize-1]\n",
    "   \n",
    "    def transition_reward(self, current_pos, action): # return the transition probability\n",
    "\n",
    "        # get next position: state: [0, 0], action: [0, 1], new_state = [0, 1]\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "\n",
    "        # normally, reward = 0\n",
    "        reward = 0\n",
    "\n",
    "        # if new pos results in off the grid, return reward -1\n",
    "        if -1 in self.new_pos or self.size in self.new_pos:\n",
    "            reward = -1\n",
    "        # if in state A, receive + 10\n",
    "        if current_pos == [0, 1]:\n",
    "            reward = 10\n",
    "        # if in state B, receive + 5\n",
    "        if current_pos == [0, 3]:\n",
    "            reward = 5\n",
    "\n",
    "        # if taking an action crosses the border; agent's new_pos is the same as the current pos\n",
    "        if -1 in self.new_pos or self.size in self.new_pos: \n",
    "            self.new_pos = current_pos\n",
    "            \n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            self.new_pos = [4, 1]\n",
    "            \n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            self.new_pos = [2, 3]\n",
    "\n",
    "        return self.new_pos, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(steps):\n",
    "\n",
    "    # set initial state\n",
    "    state_vector = grid.initial_state()\n",
    "\n",
    "    # initialize state (with iniitial state), action list and reward list\n",
    "    state_list = [state_vector]\n",
    "    action_list = []\n",
    "    reward_list = []\n",
    "\n",
    "    # generate an episode\n",
    "    for i in range(steps):\n",
    "\n",
    "        # pick an action based on categorical distribution in policy\n",
    "        action_index = int(np.random.choice(action_count, 1, p=policy[grid.states.index(state_vector)])) \n",
    "        action_vector = actions[action_index] # convert the integer index (ie. 0) to action (ie. [-1, 0])\n",
    "\n",
    "        # get new state and reward after taking action from current state\n",
    "        new_state_vector, reward = grid.transition_reward(state_vector, action_vector)\n",
    "        state_vector = list(new_state_vector)\n",
    "\n",
    "        # save state, action chosen and reward to list\n",
    "        state_list.append(state_vector)\n",
    "        action_list.append(action_vector)\n",
    "        reward_list.append(reward)\n",
    "        \n",
    "    return state_list, action_list, reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define average function\n",
    "def Average(lst): \n",
    "    return sum(lst) / len(lst) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Visit MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a grid object\n",
    "grid = Gridworld(5)\n",
    "\n",
    "# initialize q values for all state action pairs\n",
    "Q_values = np.zeros((state_count, action_count))\n",
    "\n",
    "# intialize parameters\n",
    "gamma = 0.99\n",
    "epsilon = 0.1\n",
    "\n",
    "# iterate 500 times: each time, generating an episode of 200 steps\n",
    "max_steps = 200\n",
    "\n",
    "# Define lists for plots\n",
    "average_reward_list = []\n",
    "cumulative_reward_list = []\n",
    "cumulative_reward = 0\n",
    "delta_list = []\n",
    "episode_test_reward_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a random policy\n",
    "random_policy = np.random.randint(100, size=(state_count, action_count))\n",
    "random_policy = random_policy/random_policy.sum(axis=1)[:,None]\n",
    "policy = random_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables for keeping track of time steps\n",
    "Terminal = max_steps\n",
    "t_list=[]\n",
    "for i in range(1,max_steps+1):\n",
    "    t = Terminal - i\n",
    "    t_list.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns_list={}\n",
    "# #initialize return values\n",
    "# for n in range(state_count):\n",
    "#     for m in range(action_count):\n",
    "#         returns_list[(n,m)] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Episode: 1'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities are not non-negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f62755fbd701>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# TEST POLICY after each episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;31m# Generate test trajectory with the greedy policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mstate_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_reward_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m# sum up all the rewards obtained during test trajectory and append to list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-abf628a7317f>\u001b[0m in \u001b[0;36mgenerate_episode\u001b[0;34m(steps)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# pick an action based on categorical distribution in policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0maction_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0maction_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# convert the integer index (ie. 0) to action (ie. [-1, 0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: probabilities are not non-negative"
     ]
    }
   ],
   "source": [
    "episode_length = 500\n",
    "\n",
    "# iteration 500 times\n",
    "for episode in range(episode_length):\n",
    "  \n",
    "    # generate an episode of specified step count\n",
    "    state_list, action_list, reward_list = generate_episode(max_steps)\n",
    "    \n",
    "    # calculate average reward of each episode\n",
    "    average_reward_list.append(Average(reward_list))\n",
    "    \n",
    "    # obtain cumulative reward for plotting\n",
    "    cumulative_reward = cumulative_reward + sum(reward_list)\n",
    "    cumulative_reward_list.append(cumulative_reward)\n",
    "\n",
    "    # intialize variables\n",
    "    G = 0\n",
    "    delta = 0\n",
    "    \n",
    "    # initiate returns and visited list to none\n",
    "    returns_list = []\n",
    "    visited_list = []\n",
    "    \n",
    "    returns_list={}\n",
    "    #initialize return values\n",
    "    for n in range(state_count):\n",
    "        for m in range(action_count):\n",
    "            returns_list[(n,m)] = []\n",
    "\n",
    "    # loop for each step of episode: T-1, T-2, T-3 ... 0 = 199, 198, 197 ... 0\n",
    "    for t in t_list:\n",
    "        \n",
    "        # calculate G: starting with the last reward at index t (naturally accounts for pseudocode's \"t-1\")\n",
    "        G = gamma*G + reward_list[t]\n",
    "        \n",
    "        # combine state action pair, for example, state = [0,0], action = [0,1], state_action_pair = [0,0,0,1]\n",
    "        state_action_pair = []\n",
    "        state_action_pair.extend(state_list[t])\n",
    "        state_action_pair.extend(action_list[t])\n",
    "        \n",
    "        # check if state action pair have been visited before (if not: continue, else: move to the next time step)\n",
    "        if state_action_pair not in visited_list:\n",
    "            \n",
    "            # add state action pair to visited list\n",
    "            visited_list.append(state_action_pair)\n",
    "            \n",
    "            # find state and action index, for example, converting action [-1, 0] to 0, and same for state #\n",
    "            state_index = grid.states.index(state_list[t])\n",
    "            action_index = actions.index(action_list[t])\n",
    "            \n",
    "            # append G to returns\n",
    "            returns_list[(state_index,action_index)].append(G)\n",
    "            \n",
    "            newQ = Average(returns_list[(state_index,action_index)])\n",
    "            \n",
    "            # calculate max delta change for plotting max q value change\n",
    "            delta = max(delta, np.abs(newQ - Q_values[state_index][action_index]))\n",
    "            \n",
    "            # write Q_values to the state-action pair\n",
    "            Q_values[state_index][action_index] = newQ\n",
    "            \n",
    "            # choose best action at given state\n",
    "            choose_action = np.argmax(Q_values[state_index])\n",
    "            \n",
    "            # if Q_values is all zero, randomly pick an action\n",
    "            if np.count_nonzero(Q_values[state_index]) == 0:\n",
    "                choose_action = random.randint(0,3)\n",
    "            \n",
    "    for s in range(state_count):\n",
    "        if np.count_nonzero(Q_values[s]) == 0:  # if Q_values is all zero, randomly pick an action\n",
    "            choose_action = random.randint(0,3)\n",
    "        else:\n",
    "            choose_action = np.argmax(Q_values[s]) # choose best action at given state\n",
    "        # overwrite policy\n",
    "        for a in range(action_count): # for action in actions [0, 1, 2, 3]\n",
    "            if choose_action == a: # if the choose_action is the same as the current action\n",
    "                # policy[state_index][a] = 1 - eps \n",
    "                policy[s][a] = 1 - episode + episode/action_count \n",
    "            else: # if choose_action is not the same as the current action\n",
    "                # policy[state_index][a] = eps/3 \n",
    "                policy[s][a] = episode/action_count\n",
    "     \n",
    "    # append delta to list\n",
    "    delta_list.append(delta)\n",
    "    \n",
    "    # TEST POLICY after each episode\n",
    "    # Generate test trajectory with the greedy policy\n",
    "    state_list, action_list, test_reward_list = generate_episode(200)\n",
    "    \n",
    "    # sum up all the rewards obtained during test trajectory and append to list\n",
    "    episode_test_reward_list.append(sum(test_reward_list))\n",
    "    \n",
    "    # print current episode\n",
    "    clear_output(wait=True)\n",
    "    display('Episode: ' + str(episode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# episode_length = 500\n",
    "\n",
    "# # iteration 500 times\n",
    "# for episode in range(episode_length):\n",
    "  \n",
    "#     # generate an episode of specified step count\n",
    "#     state_list, action_list, reward_list = generate_episode(max_steps)\n",
    "    \n",
    "#     # calculate average reward of each episode\n",
    "#     average_reward_list.append(Average(reward_list))\n",
    "    \n",
    "#     # obtain cumulative reward for plotting\n",
    "#     cumulative_reward = cumulative_reward + sum(reward_list)\n",
    "#     cumulative_reward_list.append(cumulative_reward)\n",
    "\n",
    "#     # intialize variables\n",
    "#     G = 0\n",
    "#     delta = 0\n",
    "    \n",
    "#     # initiate returns and visited list to none\n",
    "#     returns_list = []\n",
    "#     visited_list = []\n",
    "\n",
    "#     # loop for each step of episode: T-1, T-2, T-3 ... 0 = 199, 198, 197 ... 0\n",
    "#     for t in t_list:\n",
    "\n",
    "#         # calculate G: starting with the last reward at index t (naturally accounts for pseudocode's \"t-1\")\n",
    "#         G = gamma*G + reward_list[t]\n",
    "        \n",
    "#         # combine state action pair, for example, state = [0,0], action = [0,1], state_action_pair = [0,0,0,1]\n",
    "#         state_action_pair = []\n",
    "#         state_action_pair.extend(state_list[t])\n",
    "#         state_action_pair.extend(action_list[t])\n",
    "\n",
    "#         # check if state action pair have been visited before (if not: continue, else: move to the next time step)\n",
    "#         if state_action_pair not in visited_list:\n",
    "\n",
    "#             # add state action pair to visited list\n",
    "#             visited_list.append(state_action_pair)\n",
    "\n",
    "#             # append G to returns\n",
    "#             returns_list.append(G)\n",
    "\n",
    "#             # find state and action index, for example, converting action [-1, 0] to 0, and same for state #\n",
    "#             state_index = grid.states.index(state_list[t])\n",
    "#             action_index = actions.index(action_list[t])\n",
    "\n",
    "#             # calculate max delta change for plotting max q value change\n",
    "#             delta = max(delta, np.abs(Average(returns_list) - Q_values[state_index][action_index]))      \n",
    "            \n",
    "#             # write Q_values to the state-action pair\n",
    "#             Q_values[state_index][action_index] = Average(returns_list)\n",
    "\n",
    "#             # choose best action at given state\n",
    "#             choose_action = np.argmax(Q_values[state_index])\n",
    "            \n",
    "#             # if Q_values is all zero, randomly pick an action\n",
    "#             if np.count_nonzero(Q_values[state_index]) == 0:\n",
    "#                 choose_action = random.randint(0,3)\n",
    "\n",
    "#             # overwrite policy\n",
    "#             for a in range(action_count): # for action in actions [0, 1, 2, 3]\n",
    "#                 if choose_action == a: # if the choose_action is the same as the current action\n",
    "#                     # policy[state_index][a] = 1 - epsilon \n",
    "#                     policy[state_index][a] = 1 - epsilon + epsilon/action_count \n",
    "#                 else: # if choose_action is not the same as the current action\n",
    "#                     # policy[state_index][a] = epsilon/3 \n",
    "#                     policy[state_index][a] = epsilon/action_count\n",
    "     \n",
    "#     # append delta to list\n",
    "#     delta_list.append(delta)\n",
    "    \n",
    "#     # TEST POLICY after each episode\n",
    "#     # Generate test trajectory with the greedy policy\n",
    "#     state_list, action_list, test_reward_list = generate_episode(200)\n",
    "    \n",
    "#     # sum up all the rewards obtained during test trajectory and append to list\n",
    "#     episode_test_reward_list.append(sum(test_reward_list))\n",
    "    \n",
    "#     # print current episode\n",
    "#     clear_output(wait=True)\n",
    "#     display('Episode: ' + str(episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = int(episode_length/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test reward of each episode\n",
    "plt.plot(episode_test_reward_list)\n",
    "plt.title('Reward per Episode during Testing')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "\n",
    "# plot moving average\n",
    "delta_frame = pd.DataFrame(episode_test_reward_list)\n",
    "rolling_mean = delta_frame.rolling(window=window_length).mean()\n",
    "plt.plot(rolling_mean, label='Moving Average', color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max delta of each episode, where delta is the change in Q values\n",
    "plt.plot(delta_list)\n",
    "plt.title('Max Delta per Episode during Training')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Max Delta')\n",
    "\n",
    "# plot moving average\n",
    "delta_frame = pd.DataFrame(delta_list)\n",
    "rolling_mean = delta_frame.rolling(window=window_length).mean()\n",
    "plt.plot(rolling_mean, label='Moving Average', color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average reward per episode\n",
    "plt.plot(average_reward_list)\n",
    "plt.title('Average Reward per Episode during Training')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "\n",
    "# plot moving average\n",
    "reward_frame = pd.DataFrame(average_reward_list)\n",
    "rolling_mean = reward_frame.rolling(window=window_length).mean()\n",
    "plt.plot(rolling_mean, label='Moving Average', color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cumulative reward per episode\n",
    "plt.plot(cumulative_reward_list)\n",
    "plt.title('Cumulative Reward per Episode during Training')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4)\n",
    "Q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Final Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT POLICY TABLE ################################################################################\n",
    "# import pandas library\n",
    "import pandas as pd\n",
    "# define column and index\n",
    "columns=range(grid.size)\n",
    "index = range(grid.size)\n",
    "# define dataframe to represent policy table\n",
    "policy_table = pd.DataFrame(index = index, columns=columns)\n",
    "\n",
    "# iterate through policy to make a table that represents action number\n",
    "# as action name (eg. left, right, up, down)\n",
    "for state in range(len(policy)):\n",
    "    \n",
    "    # find the best action at each state\n",
    "    best_action = np.argmax(policy[state])\n",
    "\n",
    "    # get action name\n",
    "    if best_action == 0:\n",
    "        action_name = 'up'\n",
    "    elif best_action == 1:\n",
    "        action_name = 'right'\n",
    "    elif best_action == 2:\n",
    "        action_name = 'down'\n",
    "    else:\n",
    "        action_name = 'left'\n",
    "\n",
    "    # calculate the row and column coordinate of the current state number\n",
    "    row = int(state/grid.size)\n",
    "    column = round((state/grid.size - int(state/grid.size))*grid.size)\n",
    "            \n",
    "    # assign action name\n",
    "    policy_table.loc[row][column] = action_name\n",
    "\n",
    "print(\"Policy Table: \")\n",
    "print(policy_table)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
