{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld():\n",
    "    def __init__(self, gridSize):\n",
    "        self.valueMap = np.zeros((gridSize, gridSize))\n",
    "        self.states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "        self.size = gridSize\n",
    "        self.new_pos = [0, 0] # initialize new position for p_transition\n",
    "        self.transition_prob = 1 # deterministic\n",
    "    \n",
    "    def initial_state(self):        # return initial state\n",
    "        return grid.states[gridSize*gridSize-1]\n",
    "   \n",
    "    def transition_reward(self, current_pos, action): # return the transition probability\n",
    "\n",
    "        # get next position: state: [0, 0], action: [0, 1], new_state = [0, 1]\n",
    "        self.new_pos = np.array(current_pos) + np.array(action)\n",
    "\n",
    "        # normally, reward = 0\n",
    "        reward = 0\n",
    "\n",
    "        # if new pos results in off the grid, return reward -1\n",
    "        if -1 in self.new_pos or self.size in self.new_pos:\n",
    "            reward = -1\n",
    "        # if in state A, receive + 10\n",
    "        if current_pos == [0, 1]:\n",
    "            reward = 10\n",
    "        # if in state B, receive + 5\n",
    "        if current_pos == [0, 3]:\n",
    "            reward = 5\n",
    "\n",
    "        # if taking an action crosses the border; agent's new_pos is the same as the current pos\n",
    "        if -1 in self.new_pos or self.size in self.new_pos: \n",
    "            self.new_pos = current_pos\n",
    "            \n",
    "        # if in state A, transition to state A'\n",
    "        if current_pos == [0, 1]:\n",
    "            self.new_pos = [4, 1]\n",
    "            \n",
    "        # if in state B, transition to state B'\n",
    "        if current_pos == [0, 3]:\n",
    "            self.new_pos = [2, 3]\n",
    "\n",
    "        return self.new_pos, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = Gridworld(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('MC_Qvalues_0.1.pkl', 'rb') as file:\n",
    "    q_values = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -5.19535238,  -4.57143619,  -3.3967326 ,  -2.4701995 ],\n",
       "       [ -3.90827205, -36.82110202,  -4.43694344, -21.31491017],\n",
       "       [-16.95657059, -31.45208528, -18.38863922, -29.10518038],\n",
       "       [ -2.52699927,  -1.18331977,  -1.67602648,  -0.61713134],\n",
       "       [-20.85596116,  -5.14109566,   2.07023041, -16.85114546],\n",
       "       [ -6.06385245, -21.78217684, -10.71999121, -20.41932592],\n",
       "       [  6.63333333,  -1.95212333,   0.        ,  -6.78672378],\n",
       "       [-31.28784792,  -0.0933968 , -26.52182377, -40.65084391],\n",
       "       [ -0.52726906, -14.45896016,  -3.61104835,   0.21354628],\n",
       "       [  0.81737909,  -1.        ,  -1.33977826,   3.61463694],\n",
       "       [ -9.38571071,  -9.19274093, -21.47501332,  -7.83513613],\n",
       "       [ -2.18771302,  -2.98142154,  -2.56609424, -26.25963018],\n",
       "       [-41.3005006 ,  -8.32717664,  -8.84279387,  -2.69641109],\n",
       "       [  0.44118459,  -2.283798  ,  -2.37181332,  -3.60239549],\n",
       "       [ -1.9701995 ,  -1.97349967,  -5.24473063,  -8.9915337 ],\n",
       "       [-11.55625544,  -3.02668508, -12.54276655,  -3.10976169],\n",
       "       [ -2.39042526,  -2.9106066 ,  -0.932911  ,  -3.37604219],\n",
       "       [ -2.81042523, -21.00319862, -32.29491263, -29.69723137],\n",
       "       [ -9.17782833,  -3.3967326 , -27.58469589,   0.19373256],\n",
       "       [ -5.86887001,  -1.        , -15.61130703,  -4.17633192],\n",
       "       [-12.13110479, -16.15057962, -12.93995396, -18.00029599],\n",
       "       [ -2.2160784 ,  -3.06600891,  -1.36780046, -14.72117415],\n",
       "       [-31.08340994,  -9.13268419,  -0.14285714,   0.        ],\n",
       "       [  1.23468662, -22.38998874, -13.23207012, -32.98344377],\n",
       "       [-11.58543181, -13.9228815 ,  -9.33946725,   2.19834612]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = 0\n",
    "q_values[run]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Table: \n",
      "       0     1      2     3      4\n",
      "0   left    up     up  left   down\n",
      "1     up    up  right  left   left\n",
      "2   left    up   left    up     up\n",
      "3  right  down     up  left  right\n",
      "4     up  down   left    up   left\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PRINT POLICY TABLE ################################################################################\n",
    "# import pandas library\n",
    "import pandas as pd\n",
    "# define column and index\n",
    "columns=range(grid.size)\n",
    "index = range(grid.size)\n",
    "# define dataframe to represent policy table\n",
    "policy_table = pd.DataFrame(index = index, columns=columns)\n",
    "\n",
    "# iterate through policy to make a table that represents action number\n",
    "# as action name (eg. left, right, up, down)\n",
    "for state in range(len(q_values[run])):\n",
    "    \n",
    "    # find the best action at each state\n",
    "    best_action = np.argmax(q_values[run][state])\n",
    "\n",
    "    # get action name\n",
    "    if best_action == 0:\n",
    "        action_name = 'up'\n",
    "    elif best_action == 1:\n",
    "        action_name = 'right'\n",
    "    elif best_action == 2:\n",
    "        action_name = 'down'\n",
    "    else:\n",
    "        action_name = 'left'\n",
    "\n",
    "    # calculate the row and column coordinate of the current state number\n",
    "    row = int(state/grid.size)\n",
    "    column = round((state/grid.size - int(state/grid.size))*grid.size)\n",
    "            \n",
    "    # assign action name\n",
    "    policy_table.loc[row][column] = action_name\n",
    "\n",
    "print(\"Policy Table: \")\n",
    "print(policy_table)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
