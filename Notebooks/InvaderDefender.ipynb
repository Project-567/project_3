{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invader Defender "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [[-1, 0], [0, 1], [1, 0], [0, -1]] #up, right, down, left = (clockwise from up) \n",
    "action_count = len(actions) \n",
    "gridSize = 6 \n",
    "state_count = gridSize*gridSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Invader_Defender():\n",
    "    def __init__(self, gridSize):\n",
    "        self.valueMap = np.zeros((gridSize, gridSize))\n",
    "        self.states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "        self.size = gridSize\n",
    "        \n",
    "        # deterministic transition ?\n",
    "        self.transition_prob = 1 \n",
    "        \n",
    "        # initialize defender and invader states\n",
    "        self.new_state = [0, 0, 0, 0]\n",
    "        self.new_defender_state = [0, 0]\n",
    "        self.new_invader_state = [0, 0]\n",
    "        \n",
    "        # set territory state\n",
    "        self.territory_state = [4, 4]\n",
    "\n",
    "        # create a list of all possible states in the game\n",
    "        self.game_state_list = []\n",
    "        for defender_state in self.states:\n",
    "            for invader_state in self.states:\n",
    "                combined_states = defender_state + invader_state\n",
    "                self.game_state_list.append(combined_states)\n",
    "        \n",
    "        # create 2 lists of states representing defender and invader victory\n",
    "        self.defender_won = []\n",
    "        self.invader_won = []\n",
    "        \n",
    "        # create states representing defender victory\n",
    "        for defender_state in self.states:\n",
    "            for invader_state in self.states:\n",
    "                distance = np.linalg.norm(np.array(defender_state) - np.array(invader_state))\n",
    "                # if the invader is not at territory and within the capture range of defender = defender won\n",
    "                if invader_state != self.territory_state and distance <= np.sqrt(2):\n",
    "                    combined_states = defender_state + invader_state\n",
    "                    self.defender_won.append(combined_states)\n",
    "           \n",
    "        # create states representing invader victory\n",
    "        for defender_state in self.states:\n",
    "            distance = np.linalg.norm(np.array(defender_state) - np.array(self.territory_state))\n",
    "            # if the invader is at territory, and outside of the defender's capture range = invader won\n",
    "            if distance > np.sqrt(2):\n",
    "                combined_states = defender_state + self.territory_state\n",
    "                self.invader_won.append(combined_states)\n",
    "    \n",
    "    def possible_states(self):\n",
    "        \"\"\"\n",
    "        A function that returns a list of all possible states in the game\n",
    "        \"\"\"\n",
    "        return self.game_state_list\n",
    "    \n",
    "    def terminal_check(self, state):\n",
    "        \"\"\"\n",
    "        A function that checks whether the game is at a terminal state.\n",
    "        Terminal state happens when either the invader or defender has won.\n",
    "        \"\"\"\n",
    "        if state in self.defender_won:\n",
    "            status = \"Defender Won\"\n",
    "            terminal_check = True\n",
    "        elif state in self.invader_won:\n",
    "            status = \"Invader Won\"\n",
    "            terminal_check = True\n",
    "        else:\n",
    "            terminal_check = False\n",
    "            status = \"Game in Progress\"\n",
    "\n",
    "        return terminal_check, status\n",
    "    \n",
    "#     def transition_probability(self, transition):\n",
    "#         \"\"\"\n",
    "#         A function that returns the transition probability...?\n",
    "#         \"\"\"\n",
    "#         return self.transition_prob, reward\n",
    "\n",
    "    def next_state(self, current_state, defender_action, invader_action):\n",
    "        \"\"\"\n",
    "        A function that returns the next state\n",
    "        Input: current state [0,0] , defender_action [0, 1], invader_action [0,-1]\n",
    "        Output: next state array([x1,y1,x2,y2]) and reward (int)\n",
    "            - If the action takes the agent off grid, the agent remains in original state\n",
    "            - If defender won, reward is calculated based on manhattan distance between invader captured state\n",
    "            and territory\n",
    "            - If defender loss, reward is -100\n",
    "        \"\"\"\n",
    "        defender_state = []\n",
    "        invader_state = []\n",
    "        \n",
    "        # deconstruct current state [0,0,1,1] in to defender [0,0] and invader [1,1] state\n",
    "        for i in range(4):\n",
    "            if i < 2:\n",
    "                defender_state.append(current_state[i])\n",
    "            else:\n",
    "                invader_state.append(current_state[i])\n",
    "                \n",
    "        # get next state: state: [0, 0], action: [0, 1], new_state = [0, 1]\n",
    "        self.new_defender_state = list(np.array(defender_state) + np.array(defender_action))\n",
    "        self.new_invader_state = list(np.array(invader_state) + np.array(invader_action))\n",
    "\n",
    "        # if new defender states results in off the grid, return to original state\n",
    "        if -1 in self.new_defender_state or self.size in self.new_defender_state:\n",
    "            self.new_defender_state = defender_state\n",
    "        \n",
    "        # if new invader states results in off the grid, return to original state\n",
    "        if -1 in self.new_invader_state or self.size in self.new_invader_state:\n",
    "            self.new_invader_state = invader_state\n",
    "       \n",
    "        # combine the defender and invader state\n",
    "        self.new_state = self.new_defender_state\n",
    "        self.new_state.extend(self.new_invader_state)\n",
    "        \n",
    "        # calculate rewards\n",
    "        terminal, status = self.terminal_check(self.new_state)\n",
    "        if terminal == True:\n",
    "            if status == \"Defender Won\":\n",
    "                # defender reward if defender won (manhattan distance between invader captured state and territory)\n",
    "                distance_to_territory = sum(abs(np.array(self.new_invader_state) - np.array(self.territory_state)))\n",
    "                self.reward = distance_to_territory\n",
    "            else:\n",
    "                # defender reward if invader won\n",
    "                self.reward = -100\n",
    "        else:\n",
    "            self.reward = 0\n",
    "            \n",
    "        return self.new_state, self.reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "invader_defender = Invader_Defender(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state, reward = invader_defender.next_state([2,1,0,0], [-1, 0], [-1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 'Defender Won')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invader_defender.terminal_check([1, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "invader_defender = Invader_Defender(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "U = {}\n",
    "state_list = []\n",
    "listofzeros = [0.0] * len(invader_defender.game_state_list)\n",
    "\n",
    "# convert game_state_list in to a state list of tuples in order to make a dictionary\n",
    "for state in invader_defender.game_state_list:\n",
    "    state_list.append(tuple(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate params\n",
    "G = dict(zip(state_list, listofzeros))\n",
    "U[k] = dict(zip(state_list, listofzeros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "payoff = np.zeros([4,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_payoff(state):\n",
    "    \"\"\"\n",
    "    A function calculates the payoff of a specific state by iterating over every defender/invader action\n",
    "    Input: state (ie. [0,0,1,1])\n",
    "    Output: payoff = 4x4 matrix where each element represent the defender's payoff \n",
    "    when defender take i, and invader take action j\n",
    "    \"\"\"\n",
    "    payoff = np.zeros([4,4])\n",
    "    for i in range(action_count):\n",
    "        defender_action = actions[i]\n",
    "        for j in range(action_count):\n",
    "            invader_action = actions[j]\n",
    "            next_state, reward = invader_defender.next_state(state, defender_action, invader_action)\n",
    "            payoff[i, j] = reward + gamma*invader_defender.transition_prob*U[k][tuple(next_state)]\n",
    "    return payoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in invader_defender.game_state_list:\n",
    "    \n",
    "    # Build G dictionary {state: payoff (4x4)}\n",
    "    G[tuple(state)] = calculate_payoff(state)\n",
    "    \n",
    "    # calculate value of game\n",
    "    \n",
    "    \n",
    "    # calculate delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = G[tuple(state)]\n",
    "\n",
    "c = [0, 0, 0, 0, -1]\n",
    "q_neg = -1*Q       \n",
    "v_col = np.ones((4,1))\n",
    "A_p2 = np.concatenate((q_neg,v_col),1)\n",
    "b = [0, 0, 0, 0]\n",
    "Aeq = [[1, 1, 1, 1, 0]]\n",
    "beq = [[1]]\n",
    "bounds = ((0,1),(0,1),(0,1),(0,1),(None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     con: array([-5.98430416e-10])\n",
       "     fun: -2.0000000000183755\n",
       " message: 'Optimization terminated successfully.'\n",
       "     nit: 4\n",
       "   slack: array([1.13251764e-09, 1.13251764e-09, 1.13251764e-09, 1.13251764e-09])\n",
       "  status: 0\n",
       " success: True\n",
       "       x: array([2.29836791e-11, 5.00000000e-01, 5.00000000e-01, 2.29836791e-11,\n",
       "       2.00000000e+00])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import linprog\n",
    "\n",
    "res_p2 = linprog(c, A_ub=A_p2, b_ub=b, A_eq=Aeq, b_eq=beq, bounds=bounds)\n",
    "res_p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
