{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invader Defender "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import pickle\n",
    "from scipy.optimize import linprog\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "# to remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [[-1, 0], [0, 1], [1, 0], [0, -1]] #up, right, down, left = (clockwise from up) \n",
    "action_count = len(actions) \n",
    "gridSize = 6 \n",
    "state_count = gridSize*gridSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Invader_Defender():\n",
    "    def __init__(self, gridSize):\n",
    "        self.valueMap = np.zeros((gridSize, gridSize))\n",
    "        self.states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "        self.size = gridSize\n",
    "        \n",
    "        # deterministic transition ?\n",
    "        self.transition_prob = 1 \n",
    "        \n",
    "        # initialize defender and invader states\n",
    "        self.new_state = [0, 0, 0, 0]\n",
    "        self.new_defender_state = [0, 0]\n",
    "        self.new_invader_state = [0, 0]\n",
    "        \n",
    "        # set territory state\n",
    "        self.territory_state = [4, 4]\n",
    "\n",
    "        # create a list of all possible states in the game\n",
    "        self.game_state_list = []\n",
    "        for defender_state in self.states:\n",
    "            for invader_state in self.states:\n",
    "                combined_states = tuple(defender_state + invader_state)\n",
    "                self.game_state_list.append(combined_states)\n",
    "        \n",
    "        # create 2 lists of states representing defender and invader victory\n",
    "        self.defender_won = []\n",
    "        self.invader_won = []\n",
    "        \n",
    "        # create states representing defender victory\n",
    "        for defender_state in self.states:\n",
    "            for invader_state in self.states:\n",
    "                distance = np.linalg.norm(np.array(defender_state) - np.array(invader_state))\n",
    "                # if the invader is not at territory and within the capture range of defender = defender won\n",
    "                if invader_state != self.territory_state and distance <= np.sqrt(2):\n",
    "                    combined_states = defender_state + invader_state\n",
    "                    self.defender_won.append(combined_states)\n",
    "           \n",
    "        # create states representing invader victory = anytime invader is at territory\n",
    "        for defender_state in self.states:               \n",
    "            combined_states = defender_state + self.territory_state\n",
    "            self.invader_won.append(combined_states)\n",
    "    \n",
    "    def possible_states(self):\n",
    "        \"\"\"\n",
    "        A function that returns a list of all possible states in the game\n",
    "        \"\"\"\n",
    "        return self.game_state_list\n",
    "    \n",
    "    def terminal_check(self, state):\n",
    "        \"\"\"\n",
    "        A function that checks whether the game is at a terminal state.\n",
    "        Terminal state happens when either the invader or defender has won.\n",
    "        \"\"\"\n",
    "        if state in self.defender_won:\n",
    "            status = \"Defender Won\"\n",
    "            terminal_check = True\n",
    "        elif state in self.invader_won:\n",
    "            status = \"Invader Won\"\n",
    "            terminal_check = True\n",
    "        else:\n",
    "            terminal_check = False\n",
    "            status = \"Game in Progress\"\n",
    "\n",
    "        return terminal_check, status\n",
    "    \n",
    "#     def transition_probability(self, transition):\n",
    "#         \"\"\"\n",
    "#         A function that returns the transition probability...?\n",
    "#         \"\"\"\n",
    "#         return self.transition_prob, reward\n",
    "\n",
    "    def next_state(self, current_state, defender_action, invader_action):\n",
    "        \"\"\"\n",
    "        A function that returns the next state\n",
    "        Input: current state [0,0] , defender_action [0, 1], invader_action [0,-1]\n",
    "        Output: next state array([x1,y1,x2,y2]) and reward (int)\n",
    "            - If the action takes the agent off grid, the agent remains in original state\n",
    "            - If defender won, reward is calculated based on manhattan distance between invader captured state\n",
    "            and territory\n",
    "            - If defender loss, reward is -100\n",
    "        \"\"\"\n",
    "        defender_state = []\n",
    "        invader_state = []\n",
    "        \n",
    "        # deconstruct current state [0,0,1,1] in to defender [0,0] and invader [1,1] state\n",
    "        for i in range(4):\n",
    "            if i < 2:\n",
    "                defender_state.append(current_state[i])\n",
    "            else:\n",
    "                invader_state.append(current_state[i])\n",
    "                \n",
    "        # get next state: state: [0, 0], action: [0, 1], new_state = [0, 1]\n",
    "        self.new_defender_state = list(np.array(defender_state) + np.array(defender_action))\n",
    "        self.new_invader_state = list(np.array(invader_state) + np.array(invader_action))\n",
    "\n",
    "        # if new defender states results in off the grid, return to original state\n",
    "        if -1 in self.new_defender_state or self.size in self.new_defender_state:\n",
    "            self.new_defender_state = defender_state\n",
    "        \n",
    "        # if new invader states results in off the grid, return to original state\n",
    "        if -1 in self.new_invader_state or self.size in self.new_invader_state:\n",
    "            self.new_invader_state = invader_state\n",
    "       \n",
    "        # combine the defender and invader state\n",
    "        self.new_state = self.new_defender_state\n",
    "        self.new_state.extend(self.new_invader_state)\n",
    "                  \n",
    "        # new rewards: penalizing defender for every step that invader takes closer to territory\n",
    "        terminal, status = self.terminal_check(self.new_state)\n",
    "        if terminal == True:\n",
    "            if status == \"Defender Won\":\n",
    "                # defender reward if defender won (manhattan distance between invader captured state and territory)\n",
    "                distance_to_territory = sum(abs(np.array(self.new_invader_state) - np.array(self.territory_state)))\n",
    "                self.reward = distance_to_territory\n",
    "            else:\n",
    "                # defender reward if invader won\n",
    "                self.reward = -100\n",
    "        else:\n",
    "            # penalize defender for every step that invader takes closer to territory\n",
    "            invader_to_territory = sum(abs(np.array(self.new_invader_state) - np.array(self.territory_state)))\n",
    "            reward_1 = -(8 - invader_to_territory)\n",
    "          \n",
    "            # give defender reward for every step it takes closer to invader\n",
    "            new_defender_state = [self.new_defender_state[0], self.new_defender_state[1]]\n",
    "            defender_to_invader = sum(abs(np.array(self.new_invader_state) - np.array(new_defender_state)))\n",
    "            reward_2 = (10 - defender_to_invader)\n",
    "            \n",
    "            self.reward = reward_1 + reward_2\n",
    "            \n",
    "#             self.reward = reward_1\n",
    "            \n",
    "        return self.new_state, self.reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization and Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "invader_defender = Invader_Defender(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_list = []\n",
    "delta_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_value(G_state):\n",
    "    \"\"\"\n",
    "    A function that calculates the value of a game by using linear programming.\n",
    "    The value is calculated in both the defender and invader's perspective which are equal in value\n",
    "    and opposite in signs\n",
    "    Input: payoff matrix of a particular state (4x4 matrix)\n",
    "    Output: Value = scalar value of the game.\n",
    "    \"\"\"\n",
    "    \n",
    "    G_state = list(G_state)\n",
    "    \n",
    "    # defender lin prog\n",
    "    c = [0, 0, 0, 0, -1]\n",
    "    defender_q = -1*np.transpose(G_state)     \n",
    "    v_coeff = np.ones((4,1))\n",
    "    Aub = np.concatenate((defender_q,v_coeff),1)\n",
    "    b = [0, 0, 0, 0]\n",
    "    Aeq = [[1, 1, 1, 1, 0]]\n",
    "    beq = [[1.]]\n",
    "    bounds = ((0,1),(0,1),(0,1),(0,1),(None, None))\n",
    "    defender_solution = linprog(c, A_ub=Aub, b_ub=b, A_eq=Aeq, b_eq=beq, bounds=bounds, method='revised simplex')\n",
    "    \n",
    "    # invader lin prog\n",
    "    c = [0, 0, 0, 0, 1]\n",
    "    invader_q = G_state\n",
    "    w_coeff = np.ones((4,1))*-1\n",
    "    Aub = np.concatenate((invader_q,w_coeff),1)\n",
    "    invader_solution = linprog(c, A_ub=Aub, b_ub=b, A_eq=Aeq, b_eq=beq, bounds=bounds, method='revised simplex')\n",
    "        \n",
    "    # check if the linprog solution is successful or not\n",
    "    if defender_solution['status'] == 0:\n",
    "        value = defender_solution['fun']*-1\n",
    "    else:\n",
    "        value = invader_solution['fun']\n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_payoff(state):\n",
    "    \"\"\"\n",
    "    A function calculates the payoff of a specific state based on Q values\n",
    "    Input: state (ie. [0,0,1,1])\n",
    "    Output: payoff = 4x4 matrix where each element represent the defender's payoff \n",
    "    when defender take i, and invader take action j\n",
    "    \"\"\"\n",
    "    state = list(state)\n",
    "    payoff = np.zeros([4,4])\n",
    "    for i in range(action_count):\n",
    "        defender_action = i\n",
    "        for j in range(action_count):\n",
    "            invader_action = j\n",
    "            joint_action = [defender_action, invader_action]\n",
    "            state_action_pair = tuple(state + joint_action)\n",
    "            payoff[i, j] = Q[state_action_pair]\n",
    "\n",
    "    return payoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equilibrium(G_state):\n",
    "    \"\"\"\n",
    "    A function that obtains the policy for defender and invader\n",
    "    The value is calculated in both the defender and invader's perspective which are equal in value\n",
    "    and opposite in signs\n",
    "    Input: payoff matrix of a particular state (4x4 matrix)\n",
    "    Output: policy for defender and invader\n",
    "    \"\"\"\n",
    "    \n",
    "    # defender lin prog\n",
    "    c = [0, 0, 0, 0, -1]\n",
    "    defender_q = -1*np.transpose(G_state)     \n",
    "    v_coeff = np.ones((4,1))\n",
    "    Aub = np.concatenate((defender_q,v_coeff),1)\n",
    "    b = [0, 0, 0, 0]\n",
    "    Aeq = [[1, 1, 1, 1, 0]]\n",
    "    beq = [[1.]]\n",
    "    bounds = ((0,1),(0,1),(0,1),(0,1),(None, None))\n",
    "    defender_solution = linprog(c, A_ub=Aub, b_ub=b, A_eq=Aeq, b_eq=beq, bounds=bounds, method='revised simplex')\n",
    "    \n",
    "    # invader lin prog\n",
    "    c = [0, 0, 0, 0, 1]\n",
    "    invader_q = G_state\n",
    "    w_coeff = np.ones((4,1))*-1\n",
    "    Aub = np.concatenate((invader_q,w_coeff),1)\n",
    "    invader_solution = linprog(c, A_ub=Aub, b_ub=b, A_eq=Aeq, b_eq=beq, bounds=bounds, method='revised simplex')\n",
    "    \n",
    "    if defender_solution['status'] == 0:\n",
    "        defender_policy = defender_solution['x'][:4]\n",
    "    else:\n",
    "        defender_policy = np.array([0.25,0.25,0.25,0.25])\n",
    "    \n",
    "    if invader_solution['status'] == 0:\n",
    "        invader_policy = invader_solution['x'][:4]\n",
    "    else:\n",
    "        invader_policy = np.array([0.25,0.25,0.25,0.25])\n",
    "\n",
    "    return defender_policy, invader_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(defender_policy, invader_policy, epsilon):\n",
    "    \"\"\"\n",
    "    A function that choose a joint epsilon-greedy action based on defender/invader policy\n",
    "    Input: defender_policy (1x4), invader policy (1x4), and epsilon (ie. 0.3)\n",
    "    Output: joint action index = [defender action index, invader action index] = [0 to 3, 0 to 3]\n",
    "    \"\"\"\n",
    "       \n",
    "    # choose an action type: explore (0) or exploit(1)\n",
    "    action_type = int(np.random.choice(2, 1, p=[epsilon,1-epsilon]))\n",
    "    \n",
    "    # pick the best action\n",
    "    best_defender_action_index = np.argmax(defender_policy)\n",
    "    best_invader_action_index = np.argmax(invader_policy)\n",
    "    \n",
    "    if action_type == 0:\n",
    "        \n",
    "        # randomly pick an action\n",
    "        random_defender_action_index = random.choice(range(4))    \n",
    "        random_invader_action_index = random.choice(range(4))    \n",
    "\n",
    "        # while random action is the same as the best action, pick a new action\n",
    "        while random_defender_action_index == best_defender_action_index:\n",
    "            random_defender_action_index = random.choice(range(4))\n",
    "        defender_action_index = random_defender_action_index\n",
    "        \n",
    "        # while random action is the same as the best action, pick a new action\n",
    "        while random_invader_action_index == best_invader_action_index:\n",
    "            random_invader_action_index = random.choice(range(4))\n",
    "        invader_action_index = random_invader_action_index\n",
    "    \n",
    "    else:\n",
    "        defender_action_index = best_defender_action_index\n",
    "        invader_action_index = best_invader_action_index\n",
    "    \n",
    "    joint_action = [defender_action_index, invader_action_index]\n",
    "    \n",
    "    return joint_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate initial states randomly\n",
    "def generate_random_initial_states(a):\n",
    "    random_initial_states = []\n",
    "    for i in range(a):\n",
    "        random_number = random.randint(0,5)\n",
    "        random_initial_states.append(random_number)\n",
    "    return tuple(random_initial_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimax Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize params\n",
    "t = 0\n",
    "T = 200\n",
    "lr = 0.9\n",
    "gamma = 0.95\n",
    "epsilon = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Q matrix\n",
    "state_action_pair_list = []\n",
    "\n",
    "# create every possible state action pairs: \n",
    "# 1296 states * 4 defender actions * 4 invader actions = 20736 s,a pairs\n",
    "for state in invader_defender.game_state_list:\n",
    "    for defender_action in range(action_count):\n",
    "        for invader_action in range(action_count):\n",
    "            joint_action = [defender_action, invader_action]\n",
    "            state_action_pair = list(state) + joint_action\n",
    "            state_action_pair_list.append(tuple(state_action_pair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a dictionary for Q values = {(x1, y1, x2, y2, defender_action_index, invader_action_index): q_value}\n",
    "listofzeros = [0.0] * len(state_action_pair_list)\n",
    "Q = dict(zip(state_action_pair_list, listofzeros))\n",
    "\n",
    "# initialize a dictionary for G values = {(x1, y1, x2, y2): payoff_matrix}\n",
    "listofzeros = [0.0] * len(invader_defender.game_state_list)\n",
    "for state in invader_defender.game_state_list:\n",
    "    state_list.append(state)\n",
    "G = dict(zip(state_list, listofzeros))\n",
    "\n",
    "# initialize policies\n",
    "initial_policy = []\n",
    "defender_policy = {}\n",
    "invader_policy = {}\n",
    "for i in range(len(invader_defender.game_state_list)):\n",
    "    random_policy = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "    initial_policy.append(random_policy)\n",
    "defender_policy = dict(zip(state_list, initial_policy))\n",
    "invader_policy = dict(zip(state_list, initial_policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Starts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize states\n",
    "defender_state = [5,0]\n",
    "invader_state = [0,0]\n",
    "current_state = tuple(defender_state + invader_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build game based on Q value\n",
    "G[current_state] = calculate_payoff(current_state)\n",
    "G[current_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 0., 0., 0.]), array([1., 0., 0., 0.]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose a policy by solving the current game\n",
    "defender_policy[current_state], invader_policy[current_state] = equilibrium(G[current_state])\n",
    "equilibrium(G[current_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'episode: 1673 delta: 1.5226896829504608 lr: 0.7612687069910828'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0d8e07627035>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# update Q[s,a] <- Q[s,a] + lr*(reward + gamma*value(s') - Q[s,a])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_state_action_pair\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_state_action_pair\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_state_action_pair\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-67c384f753ef>\u001b[0m in \u001b[0;36mcalculate_value\u001b[0;34m(G_state)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mw_coeff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mAub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvader_q\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_coeff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0minvader_solution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinprog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_ub\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_ub\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_eq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAeq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_eq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'revised simplex'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# check if the linprog solution is successful or not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/scipy/optimize/_linprog.py\u001b[0m in \u001b[0;36mlinprog\u001b[0;34m(c, A_ub, b_ub, A_eq, b_eq, bounds, method, callback, options, x0)\u001b[0m\n\u001b[1;32m    556\u001b[0m             x, status, message, iteration = _linprog_rs(\n\u001b[1;32m    557\u001b[0m                 \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 postsolve_args=postsolve_args, **solver_options)\n\u001b[0m\u001b[1;32m    559\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown solver %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/scipy/optimize/_linprog_rs.py\u001b[0m in \u001b[0;36m_linprog_rs\u001b[0;34m(c, c0, A, b, x0, callback, postsolve_args, maxiter, tol, disp, maxupdate, mast, pivot, **unknown_options)\u001b[0m\n\u001b[1;32m    547\u001b[0m     x, basis, A, b, residual, status, iteration = (\n\u001b[1;32m    548\u001b[0m         _phase_one(A, b, x0, callback, postsolve_args,\n\u001b[0;32m--> 549\u001b[0;31m                    maxiter, tol, disp, maxupdate, mast, pivot))\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/scipy/optimize/_linprog_rs.py\u001b[0m in \u001b[0;36m_phase_one\u001b[0;34m(A, b, x0, callback, postsolve_args, maxiter, tol, disp, maxupdate, mast, pivot)\u001b[0m\n\u001b[1;32m     63\u001b[0m                                           \u001b[0mmaxiter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                                           \u001b[0mmaxupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpivot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                                           iter_k, phase_one_n)\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# check for infeasibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/scipy/optimize/_linprog_rs.py\u001b[0m in \u001b[0;36m_phase_two\u001b[0;34m(c, A, x, b, callback, postsolve_args, maxiter, tol, disp, maxupdate, mast, pivot, iteration, phase_one_n)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_select_enter_pivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# similar to u = solve(B, A[:, j])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtol\u001b[0m                 \u001b[0;31m# if none of the u are positive, unbounded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_bglu_dense.pyx\u001b[0m in \u001b[0;36mscipy.optimize._bglu_dense._consider_refactor.f\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_bglu_dense.pyx\u001b[0m in \u001b[0;36mscipy.optimize._bglu_dense.BGLU.solve\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/scipy/linalg/basic.py\u001b[0m in \u001b[0;36msolve_triangular\u001b[0;34m(a, b, trans, lower, unit_diagonal, overwrite_b, debug, check_finite)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtrans\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         x, info = trtrs(a1, b1, overwrite_b=overwrite_b, lower=lower,\n\u001b[0;32m--> 348\u001b[0;31m                         trans=trans, unitdiag=unit_diagonal)\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;31m# transposed system is solved since trtrs expects Fortran ordering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# trajectory = []\n",
    "episodes = 5000\n",
    "eps = 0\n",
    "unique_counter = 0\n",
    "tolerance = 1\n",
    "delta_list = []\n",
    "delta = 2\n",
    "\n",
    "for eps in range(episodes):\n",
    "    t = 0\n",
    "    delta = 0\n",
    "    while t < T:\n",
    "\n",
    "        # choose a joint based on epsilon greedy (joint_action = [a1_indx, a2_indx])\n",
    "        joint_action = choose_action(defender_policy[current_state], invader_policy[current_state], epsilon)\n",
    "        current_state_action_pair = tuple(list(current_state) + joint_action) # ie. (x1, y1, x2, y2, a1_indx, a2_indx)\n",
    "\n",
    "        # get next state and reward based on current state [x1,y1,x2,y2] and joint action [a1_indx, a2_indx]\n",
    "        next_state, reward = invader_defender.next_state(current_state, actions[joint_action[0]], actions[joint_action[1]])\n",
    "        next_state = tuple(next_state)\n",
    "        \n",
    "        # if game reached terminal state, restart new episode with random initial states\n",
    "        terminal, status = invader_defender.terminal_check(list(next_state))\n",
    "        if terminal:\n",
    "            defender_state = [5,0]\n",
    "            invader_state = [0,0]\n",
    "            current_state = tuple(defender_state + invader_state)\n",
    "            break\n",
    "        \n",
    "        # build a game based on next state: calculate payoff of next state\n",
    "        G[next_state] = calculate_payoff(next_state)\n",
    "\n",
    "        # generate a policy based on equilibirum of next game\n",
    "        defender_policy[next_state], invader_policy[next_state] = equilibrium(G[next_state])\n",
    "\n",
    "        # make copy of Q table\n",
    "        Q_copy = Q[current_state_action_pair]\n",
    "\n",
    "        # update Q[s,a] <- Q[s,a] + lr*(reward + gamma*value(s') - Q[s,a])\n",
    "        value = calculate_value(G[next_state])\n",
    "        Q[current_state_action_pair] = Q[current_state_action_pair] + lr*(reward + gamma*value - Q[current_state_action_pair])\n",
    "       \n",
    "        # calculate delta\n",
    "        delta = max(delta, abs(Q[current_state_action_pair] - Q_copy))\n",
    "                \n",
    "        # set next state as current state\n",
    "        current_state = next_state\n",
    "        t+=1\n",
    "    \n",
    "#     lr = lr*0.9999\n",
    "\n",
    "#     print k and current max delta\n",
    "    clear_output(wait=True)\n",
    "    display('episode: ' + str(eps) + ' delta: ' + str(delta) + ' lr: ' + str(lr))\n",
    "    delta_list.append(delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(delta_list)\n",
    "plt.title('Q Delta with Epsilon: ' + str(epsilon))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Delta')\n",
    "\n",
    "# plot moving average\n",
    "reward_frame = pd.DataFrame(delta_list)\n",
    "rolling_mean = reward_frame.rolling(window=50).mean()\n",
    "plt.plot(rolling_mean, label='Moving Average', color='orange')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy extraction\n",
    "for state in invader_defender.game_state_list:    \n",
    "    G[state] = calculate_payoff(state)\n",
    "    defender_policy[state], invader_policy[state] = equilibrium(G[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defender_policy[(5,0,0,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invader_policy[(5,0,0,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defender_policy[(5,0,1,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invader_policy[(5,0,1,2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results to Pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Q_minimax.pickle', 'wb') as handle:\n",
    "#     pickle.dump(Q, handle)\n",
    "\n",
    "# with open('G_minimax.pickle', 'wb') as handle:\n",
    "#     pickle.dump(G, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from Pickle and Extract Policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment to load U and G\n",
    "# with open ('Q_minimax.pickle', 'rb') as handle:\n",
    "#     Q = pickle.load(handle)\n",
    "    \n",
    "# with open ('G_minimax.pickle', 'rb') as handle:\n",
    "#     G = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize policies\n",
    "# initial_policy = []\n",
    "# defender_policy = {}\n",
    "# invader_policy = {}\n",
    "# for i in range(len(invader_defender.game_state_list)):\n",
    "#     random_policy = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "#     initial_policy.append(random_policy)\n",
    "# defender_policy = dict(zip(state_list, initial_policy))\n",
    "# invader_policy = dict(zip(state_list, initial_policy))\n",
    "\n",
    "# # policy extraction\n",
    "# for state in invader_defender.game_state_list:    \n",
    "#     G[state] = calculate_payoff(state)\n",
    "#     defender_policy[state], invader_policy[state] = equilibrium(G[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defender_policy[(5,0,0,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invader_policy[(5,0,0,0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Q list\n",
    "\n",
    "Q_state_list = []\n",
    "\n",
    "for state in invader_defender.game_state_list:\n",
    "    Q_state = []\n",
    "    for defender_action in range(action_count):\n",
    "        for invader_action in range(action_count):\n",
    "            state_action_pair = tuple(list(state) + [defender_action, invader_action])\n",
    "#             print(state_action_pair)\n",
    "            Q_state.append(Q[state_action_pair])\n",
    "    Q_state_list.append(Q_state)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_state_dict = dict(zip(state_list, Q_state_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invader Map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of states that fixes the defender's starting position\n",
    "fixed_defender_state_list = []\n",
    "for invader_state in invader_defender.states:\n",
    "    fixed_defender_state = tuple([5, 0] + invader_state)\n",
    "    fixed_defender_state_list.append(fixed_defender_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invader_map = np.zeros([6,6])\n",
    "for state in fixed_defender_state_list:\n",
    "    invader_map[state[2], state[3]] = max(Q_state_dict[state])*-1  # -1 for invaders perspective\n",
    "\n",
    "# if the defender is fixed at the bottom left corner, this heatmap shows the invader's rewards\n",
    "plt.imshow(invader_map, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Value Function from the Invader Perspective (Defender fixed at [5,0])')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defender Map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of states that fixes the invaders's starting position\n",
    "fixed_invader_state_list = []\n",
    "for defender_state in invader_defender.states:\n",
    "    fixed_invader_state = tuple(defender_state + [0, 0])\n",
    "    fixed_invader_state_list.append(fixed_invader_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create invader heatmap\n",
    "defender_map = np.zeros([6,6])\n",
    "for state in fixed_invader_state_list:\n",
    "    defender_map[state[0], state[1]] = max(Q_state_dict[state])\n",
    "\n",
    "# if invader is fixed at top left corner, this heatmap shows the defender's rewards\n",
    "plt.imshow(defender_map, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Value Function from the Defender Perspective (Invader fixed at [0,0])')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play a Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(Defender_state, Invader_state):\n",
    "    game_trajectory = []\n",
    "    terminal = False\n",
    "    generated = False\n",
    "    \n",
    "    # while not successful generation, repeat\n",
    "    while not generated:\n",
    "        game_step = 0\n",
    "        current_state = tuple(Defender_state + Invader_state)\n",
    "\n",
    "        # generate a game trajectory\n",
    "        while not terminal:\n",
    "            \n",
    "            generated = True\n",
    "            \n",
    "            # append game trajectory\n",
    "            game_trajectory.append(current_state)\n",
    "\n",
    "            # check if game is terminal (someone won)\n",
    "            terminal, status = invader_defender.terminal_check(list(current_state))\n",
    "\n",
    "            # both agents choose action based on policy via sampling\n",
    "            invader_action = actions[int(np.random.choice(action_count, 1, p=invader_policy[tuple(current_state)]))]\n",
    "            defender_action = actions[int(np.random.choice(action_count, 1, p=defender_policy[tuple(current_state)]))]\n",
    "\n",
    "            # obtain next state\n",
    "            next_state, reward = invader_defender.next_state(list(current_state), defender_action, invader_action)\n",
    "            current_state = tuple(next_state)\n",
    "\n",
    "            game_step += 1\n",
    "            clear_output(wait=True)\n",
    "            display(\"game step: \" + str(game_step))\n",
    "            \n",
    "            # exit the game if the game steps increase pass 100 (implying agents are stuck)\n",
    "            if game_step > 100:\n",
    "                generated = False\n",
    "                break\n",
    "    \n",
    "    return game_trajectory, status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animate the Game "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate game trajectory\n",
    "game_trajectory, status = generate_trajectory([5,0],[0,0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define game dimensions\n",
    "columns=range(invader_defender.size)\n",
    "index = range(invader_defender.size)\n",
    "\n",
    "# animate the game\n",
    "for step in range(len(game_trajectory)):\n",
    "    game_table = pd.DataFrame(0, index = index, columns=columns)\n",
    "    game_table[4][4] = 'Ter.'\n",
    "    game_table[game_trajectory[step][1]][game_trajectory[step][0]] = 'DEF'\n",
    "    game_table[game_trajectory[step][3]][game_trajectory[step][2]] = 'INV'\n",
    "    clear_output(wait=True)\n",
    "    display(game_table)\n",
    "    time.sleep(0.2)\n",
    "    \n",
    "# print game status\n",
    "display(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
