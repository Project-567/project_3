{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invader Defender "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import pickle\n",
    "from scipy.optimize import linprog\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "# to remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [[-1, 0], [0, 1], [1, 0], [0, -1]] #up, right, down, left = (clockwise from up) \n",
    "action_count = len(actions) \n",
    "gridSize = 6 \n",
    "state_count = gridSize*gridSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Invader_Defender():\n",
    "    def __init__(self, gridSize):\n",
    "        self.valueMap = np.zeros((gridSize, gridSize))\n",
    "        self.states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "        self.size = gridSize\n",
    "        \n",
    "        # deterministic transition ?\n",
    "        self.transition_prob = 1 \n",
    "        \n",
    "        # initialize defender and invader states\n",
    "        self.new_state = [0, 0, 0, 0]\n",
    "        self.new_defender_state = [0, 0]\n",
    "        self.new_invader_state = [0, 0]\n",
    "        \n",
    "        # set territory state\n",
    "        self.territory_state = [4, 4]\n",
    "\n",
    "        # create a list of all possible states in the game\n",
    "        self.game_state_list = []\n",
    "        for defender_state in self.states:\n",
    "            for invader_state in self.states:\n",
    "                combined_states = tuple(defender_state + invader_state)\n",
    "                self.game_state_list.append(combined_states)\n",
    "        \n",
    "        # create 2 lists of states representing defender and invader victory\n",
    "        self.defender_won = []\n",
    "        self.invader_won = []\n",
    "        \n",
    "        # create states representing defender victory\n",
    "        for defender_state in self.states:\n",
    "            for invader_state in self.states:\n",
    "                distance = np.linalg.norm(np.array(defender_state) - np.array(invader_state))\n",
    "                # if the invader is not at territory and within the capture range of defender = defender won\n",
    "                if invader_state != self.territory_state and distance <= np.sqrt(2):\n",
    "                    combined_states = defender_state + invader_state\n",
    "                    self.defender_won.append(combined_states)\n",
    "           \n",
    "        # create states representing invader victory = anytime invader is at territory\n",
    "        for defender_state in self.states:               \n",
    "            combined_states = defender_state + self.territory_state\n",
    "            self.invader_won.append(combined_states)\n",
    "    \n",
    "    def possible_states(self):\n",
    "        \"\"\"\n",
    "        A function that returns a list of all possible states in the game\n",
    "        \"\"\"\n",
    "        return self.game_state_list\n",
    "    \n",
    "    def terminal_check(self, state):\n",
    "        \"\"\"\n",
    "        A function that checks whether the game is at a terminal state.\n",
    "        Terminal state happens when either the invader or defender has won.\n",
    "        \"\"\"\n",
    "        if state in self.defender_won:\n",
    "            status = \"Defender Won\"\n",
    "            terminal_check = True\n",
    "        elif state in self.invader_won:\n",
    "            status = \"Invader Won\"\n",
    "            terminal_check = True\n",
    "        else:\n",
    "            terminal_check = False\n",
    "            status = \"Game in Progress\"\n",
    "\n",
    "        return terminal_check, status\n",
    "    \n",
    "#     def transition_probability(self, transition):\n",
    "#         \"\"\"\n",
    "#         A function that returns the transition probability...?\n",
    "#         \"\"\"\n",
    "#         return self.transition_prob, reward\n",
    "\n",
    "    def next_state(self, current_state, defender_action, invader_action):\n",
    "        \"\"\"\n",
    "        A function that returns the next state\n",
    "        Input: current state [0,0] , defender_action [0, 1], invader_action [0,-1]\n",
    "        Output: next state array([x1,y1,x2,y2]) and reward (int)\n",
    "            - If the action takes the agent off grid, the agent remains in original state\n",
    "            - If defender won, reward is calculated based on manhattan distance between invader captured state\n",
    "            and territory\n",
    "            - If defender loss, reward is -100\n",
    "        \"\"\"\n",
    "        defender_state = []\n",
    "        invader_state = []\n",
    "        \n",
    "        # deconstruct current state [0,0,1,1] in to defender [0,0] and invader [1,1] state\n",
    "        for i in range(4):\n",
    "            if i < 2:\n",
    "                defender_state.append(current_state[i])\n",
    "            else:\n",
    "                invader_state.append(current_state[i])\n",
    "                \n",
    "        # get next state: state: [0, 0], action: [0, 1], new_state = [0, 1]\n",
    "        self.new_defender_state = list(np.array(defender_state) + np.array(defender_action))\n",
    "        self.new_invader_state = list(np.array(invader_state) + np.array(invader_action))\n",
    "\n",
    "        # if new defender states results in off the grid, return to original state\n",
    "        if -1 in self.new_defender_state or self.size in self.new_defender_state:\n",
    "            self.new_defender_state = defender_state\n",
    "        \n",
    "        # if new invader states results in off the grid, return to original state\n",
    "        if -1 in self.new_invader_state or self.size in self.new_invader_state:\n",
    "            self.new_invader_state = invader_state\n",
    "       \n",
    "        # combine the defender and invader state\n",
    "        self.new_state = self.new_defender_state\n",
    "        self.new_state.extend(self.new_invader_state)\n",
    "                  \n",
    "        # new rewards: penalizing defender for every step that invader takes closer to territory\n",
    "        terminal, status = self.terminal_check(self.new_state)\n",
    "        if terminal == True:\n",
    "            if status == \"Defender Won\":\n",
    "                # defender reward if defender won (manhattan distance between invader captured state and territory)\n",
    "                distance_to_territory = sum(abs(np.array(self.new_invader_state) - np.array(self.territory_state)))\n",
    "                self.reward = distance_to_territory\n",
    "            else:\n",
    "                # defender reward if invader won\n",
    "                self.reward = -100\n",
    "        else:\n",
    "            # penalize defender for every step that invader takes closer to territory\n",
    "            invader_to_territory = sum(abs(np.array(self.new_invader_state) - np.array(self.territory_state)))\n",
    "            reward_1 = -(8 - invader_to_territory)\n",
    "          \n",
    "            # give defender reward for every step it takes closer to invader\n",
    "            new_defender_state = [self.new_defender_state[0], self.new_defender_state[1]]\n",
    "            defender_to_invader = sum(abs(np.array(self.new_invader_state) - np.array(new_defender_state)))\n",
    "            reward_2 = (10 - defender_to_invader)\n",
    "            \n",
    "            self.reward = reward_1 + reward_2\n",
    "            \n",
    "#             self.reward = reward_1\n",
    "            \n",
    "        return self.new_state, self.reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization and Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "invader_defender = Invader_Defender(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_list = []\n",
    "delta_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_value(G_state):\n",
    "    \"\"\"\n",
    "    A function that calculates the value of a game by using linear programming.\n",
    "    The value is calculated in both the defender and invader's perspective which are equal in value\n",
    "    and opposite in signs\n",
    "    Input: payoff matrix of a particular state (4x4 matrix)\n",
    "    Output: Value = scalar value of the game.\n",
    "    \"\"\"\n",
    "    \n",
    "    G_state = list(G_state)\n",
    "    \n",
    "    # defender lin prog\n",
    "    c = [0, 0, 0, 0, -1]\n",
    "    defender_q = -1*np.transpose(G_state)     \n",
    "    v_coeff = np.ones((4,1))\n",
    "    Aub = np.concatenate((defender_q,v_coeff),1)\n",
    "    b = [0, 0, 0, 0]\n",
    "    Aeq = [[1, 1, 1, 1, 0]]\n",
    "    beq = [[1.]]\n",
    "    bounds = ((0,1),(0,1),(0,1),(0,1),(None, None))\n",
    "    defender_solution = linprog(c, A_ub=Aub, b_ub=b, A_eq=Aeq, b_eq=beq, bounds=bounds, method='revised simplex')\n",
    "    \n",
    "    # invader lin prog\n",
    "    c = [0, 0, 0, 0, 1]\n",
    "    invader_q = G_state\n",
    "    w_coeff = np.ones((4,1))*-1\n",
    "    Aub = np.concatenate((invader_q,w_coeff),1)\n",
    "    invader_solution = linprog(c, A_ub=Aub, b_ub=b, A_eq=Aeq, b_eq=beq, bounds=bounds, method='revised simplex')\n",
    "        \n",
    "    # check if the linprog solution is successful or not\n",
    "    if defender_solution['status'] == 0:\n",
    "        value = defender_solution['fun']*-1\n",
    "    else:\n",
    "        value = invader_solution['fun']\n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_payoff(state):\n",
    "    \"\"\"\n",
    "    A function calculates the payoff of a specific state based on Q values\n",
    "    Input: state (ie. [0,0,1,1])\n",
    "    Output: payoff = 4x4 matrix where each element represent the defender's payoff \n",
    "    when defender take i, and invader take action j\n",
    "    \"\"\"\n",
    "    state = list(state)\n",
    "    payoff = np.zeros([4,4])\n",
    "    for i in range(action_count):\n",
    "        defender_action = i\n",
    "        for j in range(action_count):\n",
    "            invader_action = j\n",
    "            joint_action = [defender_action, invader_action]\n",
    "            state_action_pair = tuple(state + joint_action)\n",
    "            payoff[i, j] = Q[state_action_pair]\n",
    "\n",
    "    return payoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equilibrium(G_state):\n",
    "    \"\"\"\n",
    "    A function that obtains the policy for defender and invader\n",
    "    The value is calculated in both the defender and invader's perspective which are equal in value\n",
    "    and opposite in signs\n",
    "    Input: payoff matrix of a particular state (4x4 matrix)\n",
    "    Output: policy for defender and invader\n",
    "    \"\"\"\n",
    "    \n",
    "    # defender lin prog\n",
    "    c = [0, 0, 0, 0, -1]\n",
    "    defender_q = -1*np.transpose(G_state)     \n",
    "    v_coeff = np.ones((4,1))\n",
    "    Aub = np.concatenate((defender_q,v_coeff),1)\n",
    "    b = [0, 0, 0, 0]\n",
    "    Aeq = [[1, 1, 1, 1, 0]]\n",
    "    beq = [[1.]]\n",
    "    bounds = ((0,1),(0,1),(0,1),(0,1),(None, None))\n",
    "    defender_solution = linprog(c, A_ub=Aub, b_ub=b, A_eq=Aeq, b_eq=beq, bounds=bounds, method='revised simplex')\n",
    "    \n",
    "    # invader lin prog\n",
    "    c = [0, 0, 0, 0, 1]\n",
    "    invader_q = G_state\n",
    "    w_coeff = np.ones((4,1))*-1\n",
    "    Aub = np.concatenate((invader_q,w_coeff),1)\n",
    "    invader_solution = linprog(c, A_ub=Aub, b_ub=b, A_eq=Aeq, b_eq=beq, bounds=bounds, method='revised simplex')\n",
    "    \n",
    "    if defender_solution['status'] == 0:\n",
    "        defender_policy = defender_solution['x'][:4]\n",
    "    else:\n",
    "        defender_policy = np.array([0.25,0.25,0.25,0.25])\n",
    "    \n",
    "    if invader_solution['status'] == 0:\n",
    "        invader_policy = invader_solution['x'][:4]\n",
    "    else:\n",
    "        invader_policy = np.array([0.25,0.25,0.25,0.25])\n",
    "\n",
    "    return defender_policy, invader_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(defender_policy, invader_policy, epsilon):\n",
    "    \"\"\"\n",
    "    A function that choose a joint epsilon-greedy action based on defender/invader policy\n",
    "    Input: defender_policy (1x4), invader policy (1x4), and epsilon (ie. 0.3)\n",
    "    Output: joint action index = [defender action index, invader action index] = [0 to 3, 0 to 3]\n",
    "    \"\"\"\n",
    "       \n",
    "    # choose an action type: explore (0) or exploit(1)\n",
    "    action_type = int(np.random.choice(2, 1, p=[epsilon,1-epsilon]))\n",
    "    \n",
    "    # pick the best action\n",
    "    best_defender_action_index = np.argmax(defender_policy)\n",
    "    best_invader_action_index = np.argmax(invader_policy)\n",
    "    \n",
    "    if action_type == 0:\n",
    "        \n",
    "        # randomly pick an action\n",
    "        random_defender_action_index = random.choice(range(4))    \n",
    "        random_invader_action_index = random.choice(range(4))    \n",
    "\n",
    "        # while random action is the same as the best action, pick a new action\n",
    "        while random_defender_action_index == best_defender_action_index:\n",
    "            random_defender_action_index = random.choice(range(4))\n",
    "        defender_action_index = random_defender_action_index\n",
    "        \n",
    "        # while random action is the same as the best action, pick a new action\n",
    "        while random_invader_action_index == best_invader_action_index:\n",
    "            random_invader_action_index = random.choice(range(4))\n",
    "        invader_action_index = random_invader_action_index\n",
    "    \n",
    "    else:\n",
    "        defender_action_index = best_defender_action_index\n",
    "        invader_action_index = best_invader_action_index\n",
    "    \n",
    "    joint_action = [defender_action_index, invader_action_index]\n",
    "    \n",
    "    return joint_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate initial states randomly\n",
    "def generate_random_initial_states(a):\n",
    "    random_initial_states = []\n",
    "    for i in range(a):\n",
    "        random_number = random.randint(0,5)\n",
    "        random_initial_states.append(random_number)\n",
    "    return tuple(random_initial_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimax Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize params\n",
    "t = 0\n",
    "T = 200\n",
    "lr = 0.9\n",
    "gamma = 0.95\n",
    "epsilon = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Q matrix\n",
    "state_action_pair_list = []\n",
    "\n",
    "# create every possible state action pairs: \n",
    "# 1296 states * 4 defender actions * 4 invader actions = 20736 s,a pairs\n",
    "for state in invader_defender.game_state_list:\n",
    "    for defender_action in range(action_count):\n",
    "        for invader_action in range(action_count):\n",
    "            joint_action = [defender_action, invader_action]\n",
    "            state_action_pair = list(state) + joint_action\n",
    "            state_action_pair_list.append(tuple(state_action_pair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a dictionary for Q values = {(x1, y1, x2, y2, defender_action_index, invader_action_index): q_value}\n",
    "listofzeros = [0.0] * len(state_action_pair_list)\n",
    "Q = dict(zip(state_action_pair_list, listofzeros))\n",
    "\n",
    "# initialize a dictionary for G values = {(x1, y1, x2, y2): payoff_matrix}\n",
    "listofzeros = [0.0] * len(invader_defender.game_state_list)\n",
    "for state in invader_defender.game_state_list:\n",
    "    state_list.append(state)\n",
    "G = dict(zip(state_list, listofzeros))\n",
    "\n",
    "# initialize policies\n",
    "initial_policy = []\n",
    "defender_policy = {}\n",
    "invader_policy = {}\n",
    "for i in range(len(invader_defender.game_state_list)):\n",
    "    random_policy = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "    initial_policy.append(random_policy)\n",
    "defender_policy = dict(zip(state_list, initial_policy))\n",
    "invader_policy = dict(zip(state_list, initial_policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Starts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize states\n",
    "# defender_state = [5,0]\n",
    "# invader_state = [0,0]\n",
    "# current_state = tuple(defender_state + invader_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # build game based on Q value\n",
    "# G[current_state] = calculate_payoff(current_state)\n",
    "# G[current_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # choose a policy by solving the current game\n",
    "# defender_policy[current_state], invader_policy[current_state] = equilibrium(G[current_state])\n",
    "# equilibrium(G[current_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # trajectory = []\n",
    "# episodes = 5000\n",
    "# eps = 0\n",
    "# unique_counter = 0\n",
    "# tolerance = 1\n",
    "# delta_list = []\n",
    "# delta = 2\n",
    "\n",
    "# for eps in range(episodes):\n",
    "#     t = 0\n",
    "#     delta = 0\n",
    "#     while t < T:\n",
    "\n",
    "#         # choose a joint based on epsilon greedy (joint_action = [a1_indx, a2_indx])\n",
    "#         joint_action = choose_action(defender_policy[current_state], invader_policy[current_state], epsilon)\n",
    "#         current_state_action_pair = tuple(list(current_state) + joint_action) # ie. (x1, y1, x2, y2, a1_indx, a2_indx)\n",
    "\n",
    "#         # get next state and reward based on current state [x1,y1,x2,y2] and joint action [a1_indx, a2_indx]\n",
    "#         next_state, reward = invader_defender.next_state(current_state, actions[joint_action[0]], actions[joint_action[1]])\n",
    "#         next_state = tuple(next_state)\n",
    "        \n",
    "#         # if game reached terminal state, restart new episode with random initial states\n",
    "#         terminal, status = invader_defender.terminal_check(list(next_state))\n",
    "#         if terminal:\n",
    "#             defender_state = [5,0]\n",
    "#             invader_state = [0,0]\n",
    "#             current_state = tuple(defender_state + invader_state)\n",
    "#             break\n",
    "        \n",
    "#         # build a game based on next state: calculate payoff of next state\n",
    "#         G[next_state] = calculate_payoff(next_state)\n",
    "\n",
    "#         # generate a policy based on equilibirum of next game\n",
    "#         defender_policy[next_state], invader_policy[next_state] = equilibrium(G[next_state])\n",
    "\n",
    "#         # make copy of Q table\n",
    "#         Q_copy = Q[current_state_action_pair]\n",
    "\n",
    "#         # update Q[s,a] <- Q[s,a] + lr*(reward + gamma*value(s') - Q[s,a])\n",
    "#         value = calculate_value(G[next_state])\n",
    "#         Q[current_state_action_pair] = Q[current_state_action_pair] + lr*(reward + gamma*value - Q[current_state_action_pair])\n",
    "       \n",
    "#         # calculate delta\n",
    "#         delta = max(delta, abs(Q[current_state_action_pair] - Q_copy))\n",
    "                \n",
    "#         # set next state as current state\n",
    "#         current_state = next_state\n",
    "#         t+=1\n",
    "    \n",
    "#     lr = lr*0.9999\n",
    "\n",
    "# #     print k and current max delta\n",
    "#     clear_output(wait=True)\n",
    "#     display('episode: ' + str(eps) + ' delta: ' + str(delta) + ' lr: ' + str(lr))\n",
    "#     delta_list.append(delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(delta_list)\n",
    "# plt.title('Q Delta with Epsilon: ' + str(epsilon))\n",
    "# plt.xlabel('Episode')\n",
    "# plt.ylabel('Delta')\n",
    "\n",
    "# # plot moving average\n",
    "# reward_frame = pd.DataFrame(delta_list)\n",
    "# rolling_mean = reward_frame.rolling(window=100).mean()\n",
    "# plt.plot(rolling_mean, label='Moving Average', color='orange')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # policy extraction\n",
    "# for state in invader_defender.game_state_list:    \n",
    "#     G[state] = calculate_payoff(state)\n",
    "#     defender_policy[state], invader_policy[state] = equilibrium(G[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defender_policy[(5,0,0,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invader_policy[(5,0,0,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defender_policy[(5,0,1,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invader_policy[(5,0,1,2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results to Pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Q_minimax.pickle', 'wb') as handle:\n",
    "#     pickle.dump(Q, handle)\n",
    "\n",
    "# with open('G_minimax.pickle', 'wb') as handle:\n",
    "#     pickle.dump(G, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from Pickle and Extract Policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to load U and G\n",
    "with open ('Q_minimax.pickle', 'rb') as handle:\n",
    "    Q = pickle.load(handle)\n",
    "    \n",
    "with open ('G_minimax.pickle', 'rb') as handle:\n",
    "    G = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize policies\n",
    "initial_policy = []\n",
    "defender_policy = {}\n",
    "invader_policy = {}\n",
    "for i in range(len(invader_defender.game_state_list)):\n",
    "    random_policy = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "    initial_policy.append(random_policy)\n",
    "defender_policy = dict(zip(state_list, initial_policy))\n",
    "invader_policy = dict(zip(state_list, initial_policy))\n",
    "\n",
    "# policy extraction\n",
    "for state in invader_defender.game_state_list:    \n",
    "    G[state] = calculate_payoff(state)\n",
    "    defender_policy[state], invader_policy[state] = equilibrium(G[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.98781088, 0.01218912, 0.        ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defender_policy[(5,0,0,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15924861, 0.84075139, 0.        , 0.        ])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invader_policy[(5,0,0,0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Q list\n",
    "\n",
    "Q_state_list = []\n",
    "\n",
    "for state in invader_defender.game_state_list:\n",
    "    Q_state = []\n",
    "    for defender_action in range(action_count):\n",
    "        for invader_action in range(action_count):\n",
    "            state_action_pair = tuple(list(state) + [defender_action, invader_action])\n",
    "#             print(state_action_pair)\n",
    "            Q_state.append(Q[state_action_pair])\n",
    "    Q_state_list.append(Q_state)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_state_dict = dict(zip(state_list, Q_state_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.794570583707458"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q[(5,0,0,0,0,0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invader Map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of states that fixes the defender's starting position\n",
    "fixed_defender_state_list = []\n",
    "for invader_state in invader_defender.states:\n",
    "    fixed_defender_state = tuple([5, 0] + invader_state)\n",
    "    fixed_defender_state_list.append(fixed_defender_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "invader_map = np.zeros([6,6])\n",
    "for state in fixed_defender_state_list:\n",
    "    invader_map[state[2], state[3]] = max(Q_state_dict[state])*-1  # -1 for invaders perspective\n",
    "\n",
    "# if the defender is fixed at the bottom left corner, this heatmap shows the invader's rewards\n",
    "plt.imshow(invader_map, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Value Function from the Invader Perspective (Defender fixed at [5,0])')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defender Map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of states that fixes the invaders's starting position\n",
    "fixed_invader_state_list = []\n",
    "for defender_state in invader_defender.states:\n",
    "    fixed_invader_state = tuple(defender_state + [0, 0])\n",
    "    fixed_invader_state_list.append(fixed_invader_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create invader heatmap\n",
    "defender_map = np.zeros([6,6])\n",
    "for state in fixed_invader_state_list:\n",
    "    defender_map[state[0], state[1]] = max(Q_state_dict[state])\n",
    "\n",
    "# if invader is fixed at top left corner, this heatmap shows the defender's rewards\n",
    "plt.imshow(defender_map, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Value Function from the Defender Perspective (Invader fixed at [0,0])')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play a Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(Defender_state, Invader_state):\n",
    "    game_trajectory = []\n",
    "    terminal = False\n",
    "    generated = False\n",
    "    \n",
    "    # while not successful generation, repeat\n",
    "    while not generated:\n",
    "        game_step = 0\n",
    "        current_state = tuple(Defender_state + Invader_state)\n",
    "\n",
    "        # generate a game trajectory\n",
    "        while not terminal:\n",
    "            \n",
    "            generated = True\n",
    "            \n",
    "            # append game trajectory\n",
    "            game_trajectory.append(current_state)\n",
    "\n",
    "            # check if game is terminal (someone won)\n",
    "            terminal, status = invader_defender.terminal_check(list(current_state))\n",
    "\n",
    "            # both agents choose action based on policy via sampling\n",
    "            invader_action = actions[int(np.random.choice(action_count, 1, p=invader_policy[tuple(current_state)]))]\n",
    "            defender_action = actions[int(np.random.choice(action_count, 1, p=defender_policy[tuple(current_state)]))]\n",
    "\n",
    "            # obtain next state\n",
    "            next_state, reward = invader_defender.next_state(list(current_state), defender_action, invader_action)\n",
    "            current_state = tuple(next_state)\n",
    "\n",
    "            game_step += 1\n",
    "            clear_output(wait=True)\n",
    "            display(\"game step: \" + str(game_step))\n",
    "            \n",
    "            # exit the game if the game steps increase pass 100 (implying agents are stuck)\n",
    "            if game_step > 100:\n",
    "                generated = False\n",
    "                break\n",
    "    \n",
    "    return game_trajectory, status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animate the Game "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'game step: 7'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate game trajectory\n",
    "game_trajectory, status = generate_trajectory([5,0],[0,0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>INV</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>DEF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ter.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2  3     4  5\n",
       "0  0    0  INV  0     0  0\n",
       "1  0  DEF    0  0     0  0\n",
       "2  0    0    0  0     0  0\n",
       "3  0    0    0  0     0  0\n",
       "4  0    0    0  0  Ter.  0\n",
       "5  0    0    0  0     0  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Defender Won'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define game dimensions\n",
    "columns=range(invader_defender.size)\n",
    "index = range(invader_defender.size)\n",
    "\n",
    "# animate the game\n",
    "for step in range(len(game_trajectory)):\n",
    "    game_table = pd.DataFrame(0, index = index, columns=columns)\n",
    "    game_table[4][4] = 'Ter.'\n",
    "    game_table[game_trajectory[step][1]][game_trajectory[step][0]] = 'DEF'\n",
    "    game_table[game_trajectory[step][3]][game_trajectory[step][2]] = 'INV'\n",
    "    clear_output(wait=True)\n",
    "    display(game_table)\n",
    "    time.sleep(0.2)\n",
    "    \n",
    "# print game status\n",
    "display(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
