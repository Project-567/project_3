{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invader Defender "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import pickle\n",
    "from scipy.optimize import linprog\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "# to remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [[-1, 0], [0, 1], [1, 0], [0, -1]] #up, right, down, left = (clockwise from up) \n",
    "action_count = len(actions) \n",
    "gridSize = 6 \n",
    "state_count = gridSize*gridSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Invader_Defender():\n",
    "    def __init__(self, gridSize):\n",
    "        self.valueMap = np.zeros((gridSize, gridSize))\n",
    "        self.states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
    "        self.size = gridSize\n",
    "        \n",
    "        # deterministic transition ?\n",
    "        self.transition_prob = 1 \n",
    "        \n",
    "        # initialize defender and invader states\n",
    "        self.new_state = [0, 0, 0, 0]\n",
    "        self.new_defender_state = [0, 0]\n",
    "        self.new_invader_state = [0, 0]\n",
    "        \n",
    "        # set territory state\n",
    "        self.territory_state = [4, 4]\n",
    "\n",
    "        # create a list of all possible states in the game\n",
    "        self.game_state_list = []\n",
    "        for defender_state in self.states:\n",
    "            for invader_state in self.states:\n",
    "                combined_states = tuple(defender_state + invader_state)\n",
    "                self.game_state_list.append(combined_states)\n",
    "        \n",
    "        # create 2 lists of states representing defender and invader victory\n",
    "        self.defender_won = []\n",
    "        self.invader_won = []\n",
    "        \n",
    "        # create states representing defender victory\n",
    "        for defender_state in self.states:\n",
    "            for invader_state in self.states:\n",
    "                distance = np.linalg.norm(np.array(defender_state) - np.array(invader_state))\n",
    "                # if the invader is not at territory and within the capture range of defender = defender won\n",
    "                if invader_state != self.territory_state and distance <= np.sqrt(2):\n",
    "                    combined_states = defender_state + invader_state\n",
    "                    self.defender_won.append(combined_states)\n",
    "           \n",
    "        # create states representing invader victory = anytime invader is at territory\n",
    "        for defender_state in self.states:               \n",
    "            combined_states = defender_state + self.territory_state\n",
    "            self.invader_won.append(combined_states)\n",
    "    \n",
    "    def possible_states(self):\n",
    "        \"\"\"\n",
    "        A function that returns a list of all possible states in the game\n",
    "        \"\"\"\n",
    "        return self.game_state_list\n",
    "    \n",
    "    def terminal_check(self, state):\n",
    "        \"\"\"\n",
    "        A function that checks whether the game is at a terminal state.\n",
    "        Terminal state happens when either the invader or defender has won.\n",
    "        \"\"\"\n",
    "        if state in self.defender_won:\n",
    "            status = \"Defender Won\"\n",
    "            terminal_check = True\n",
    "        elif state in self.invader_won:\n",
    "            status = \"Invader Won\"\n",
    "            terminal_check = True\n",
    "        else:\n",
    "            terminal_check = False\n",
    "            status = \"Game in Progress\"\n",
    "\n",
    "        return terminal_check, status\n",
    "    \n",
    "#     def transition_probability(self, transition):\n",
    "#         \"\"\"\n",
    "#         A function that returns the transition probability...?\n",
    "#         \"\"\"\n",
    "#         return self.transition_prob, reward\n",
    "\n",
    "    def next_state(self, current_state, defender_action, invader_action):\n",
    "        \"\"\"\n",
    "        A function that returns the next state\n",
    "        Input: current state [0,0] , defender_action [0, 1], invader_action [0,-1]\n",
    "        Output: next state array([x1,y1,x2,y2]) and reward (int)\n",
    "            - If the action takes the agent off grid, the agent remains in original state\n",
    "            - If defender won, reward is calculated based on manhattan distance between invader captured state\n",
    "            and territory\n",
    "            - If defender loss, reward is -100\n",
    "        \"\"\"\n",
    "        defender_state = []\n",
    "        invader_state = []\n",
    "        \n",
    "        # deconstruct current state [0,0,1,1] in to defender [0,0] and invader [1,1] state\n",
    "        for i in range(4):\n",
    "            if i < 2:\n",
    "                defender_state.append(current_state[i])\n",
    "            else:\n",
    "                invader_state.append(current_state[i])\n",
    "                \n",
    "        # get next state: state: [0, 0], action: [0, 1], new_state = [0, 1]\n",
    "        self.new_defender_state = list(np.array(defender_state) + np.array(defender_action))\n",
    "        self.new_invader_state = list(np.array(invader_state) + np.array(invader_action))\n",
    "\n",
    "        # if new defender states results in off the grid, return to original state\n",
    "        if -1 in self.new_defender_state or self.size in self.new_defender_state:\n",
    "            self.new_defender_state = defender_state\n",
    "        \n",
    "        # if new invader states results in off the grid, return to original state\n",
    "        if -1 in self.new_invader_state or self.size in self.new_invader_state:\n",
    "            self.new_invader_state = invader_state\n",
    "       \n",
    "        # combine the defender and invader state\n",
    "        self.new_state = self.new_defender_state\n",
    "        self.new_state.extend(self.new_invader_state)\n",
    "        \n",
    "#         # original rewards\n",
    "#         terminal, status = self.terminal_check(self.new_state)\n",
    "#         if terminal == True:\n",
    "#             if status == \"Defender Won\":\n",
    "#                 # defender reward if defender won (manhattan distance between invader captured state and territory)\n",
    "#                 distance_to_territory = sum(abs(np.array(self.new_invader_state) - np.array(self.territory_state)))\n",
    "#                 self.reward = distance_to_territory\n",
    "#             else:\n",
    "#                 # defender reward if invader won\n",
    "#                 self.reward = -100\n",
    "#         else:\n",
    "#             self.reward = 0\n",
    "            \n",
    "        # new rewards: penalizing defender for every step that invader takes closer to territory\n",
    "        terminal, status = self.terminal_check(self.new_state)\n",
    "        if terminal == True:\n",
    "            if status == \"Defender Won\":\n",
    "                # defender reward if defender won (manhattan distance between invader captured state and territory)\n",
    "                distance_to_territory = sum(abs(np.array(self.new_invader_state) - np.array(self.territory_state)))\n",
    "                self.reward = distance_to_territory\n",
    "            else:\n",
    "                # defender reward if invader won\n",
    "                self.reward = -100\n",
    "        else:\n",
    "            # penalize defender for every step that invader takes closer to territory\n",
    "            invader_to_territory = sum(abs(np.array(self.new_invader_state) - np.array(self.territory_state)))\n",
    "            reward_1 = -(8 - invader_to_territory)\n",
    "          \n",
    "            # give defender reward for every step it takes closer to invader\n",
    "#             new_defender_state = [self.new_defender_state[0], self.new_defender_state[1]]\n",
    "#             defender_to_invader = sum(abs(np.array(self.new_invader_state) - np.array(new_defender_state)))\n",
    "#             reward_2 = (10 - defender_to_invader)\n",
    "            \n",
    "#             self.reward = reward_1 + reward_2\n",
    "            \n",
    "            self.reward = reward_1\n",
    "            \n",
    "        return self.new_state, self.reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization and Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "invader_defender = Invader_Defender(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_list = []\n",
    "delta_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_value(G_state):\n",
    "    \"\"\"\n",
    "    A function that calculates the value of a game by using linear programming.\n",
    "    The value is calculated in both the defender and invader's perspective which are equal in value\n",
    "    and opposite in signs\n",
    "    Input: payoff matrix of a particular state (4x4 matrix)\n",
    "    Output: Value = scalar value of the game.\n",
    "    \"\"\"\n",
    "    \n",
    "    G_state = list(G_state)\n",
    "    \n",
    "    # defender lin prog\n",
    "    c = [0, 0, 0, 0, -1]\n",
    "    defender_q = -1*np.transpose(G_state)     \n",
    "    v_coeff = np.ones((4,1))\n",
    "    Aub = np.concatenate((defender_q,v_coeff),1)\n",
    "    b = [0, 0, 0, 0]\n",
    "    Aeq = [[1, 1, 1, 1, 0]]\n",
    "    beq = [[1.]]\n",
    "    bounds = ((0,1),(0,1),(0,1),(0,1),(None, None))\n",
    "    defender_solution = linprog(c, A_ub=Aub, b_ub=b, A_eq=Aeq, b_eq=beq, bounds=bounds, method='revised simplex')\n",
    "    \n",
    "    # invader lin prog\n",
    "    c = [0, 0, 0, 0, 1]\n",
    "    invader_q = G_state\n",
    "    w_coeff = np.ones((4,1))*-1\n",
    "    Aub = np.concatenate((invader_q,w_coeff),1)\n",
    "    invader_solution = linprog(c, A_ub=Aub, b_ub=b, A_eq=Aeq, b_eq=beq, bounds=bounds, method='revised simplex')\n",
    "        \n",
    "    # check if the linprog solution is successful or not\n",
    "    if defender_solution['status'] == 0:\n",
    "        value = defender_solution['fun']*-1\n",
    "    else:\n",
    "        value = invader_solution['fun']\n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_payoff(state):\n",
    "    \"\"\"\n",
    "    A function calculates the payoff of a specific state based on Q values\n",
    "    Input: state (ie. [0,0,1,1])\n",
    "    Output: payoff = 4x4 matrix where each element represent the defender's payoff \n",
    "    when defender take i, and invader take action j\n",
    "    \"\"\"\n",
    "    state = list(state)\n",
    "    payoff = np.zeros([4,4])\n",
    "    for i in range(action_count):\n",
    "        defender_action = i\n",
    "        for j in range(action_count):\n",
    "            invader_action = j\n",
    "            joint_action = [defender_action, invader_action]\n",
    "            state_action_pair = tuple(state + joint_action)\n",
    "            payoff[i, j] = Q[state_action_pair]\n",
    "\n",
    "    return payoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equilibrium(G_state):\n",
    "    \"\"\"\n",
    "    A function that obtains the policy for defender and invader\n",
    "    The value is calculated in both the defender and invader's perspective which are equal in value\n",
    "    and opposite in signs\n",
    "    Input: payoff matrix of a particular state (4x4 matrix)\n",
    "    Output: policy for defender and invader\n",
    "    \"\"\"\n",
    "    \n",
    "    # defender lin prog\n",
    "    c = [0, 0, 0, 0, -1]\n",
    "    defender_q = -1*np.transpose(G_state)     \n",
    "    v_coeff = np.ones((4,1))\n",
    "    Aub = np.concatenate((defender_q,v_coeff),1)\n",
    "    b = [0, 0, 0, 0]\n",
    "    Aeq = [[1, 1, 1, 1, 0]]\n",
    "    beq = [[1.]]\n",
    "    bounds = ((0,1),(0,1),(0,1),(0,1),(None, None))\n",
    "    defender_solution = linprog(c, A_ub=Aub, b_ub=b, A_eq=Aeq, b_eq=beq, bounds=bounds, method='revised simplex')\n",
    "    \n",
    "    # invader lin prog\n",
    "    c = [0, 0, 0, 0, 1]\n",
    "    invader_q = G_state\n",
    "    w_coeff = np.ones((4,1))*-1\n",
    "    Aub = np.concatenate((invader_q,w_coeff),1)\n",
    "    invader_solution = linprog(c, A_ub=Aub, b_ub=b, A_eq=Aeq, b_eq=beq, bounds=bounds, method='revised simplex')\n",
    "    \n",
    "    if defender_solution['status'] == 0:\n",
    "        defender_policy = defender_solution['x'][:4]\n",
    "    else:\n",
    "        defender_policy = np.array([0.25,0.25,0.25,0.25])\n",
    "    \n",
    "    if invader_solution['status'] == 0:\n",
    "        invader_policy = invader_solution['x'][:4]\n",
    "    else:\n",
    "        invader_policy = np.array([0.25,0.25,0.25,0.25])\n",
    "\n",
    "    return defender_policy, invader_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(defender_policy, invader_policy, epsilon):\n",
    "    \"\"\"\n",
    "    A function that choose a joint epsilon-greedy action based on defender/invader policy\n",
    "    Input: defender_policy (1x4), invader policy (1x4), and epsilon (ie. 0.3)\n",
    "    Output: joint action index = [defender action index, invader action index] = [0 to 3, 0 to 3]\n",
    "    \"\"\"\n",
    "       \n",
    "    # choose an action type: explore (0) or exploit(1)\n",
    "    action_type = int(np.random.choice(2, 1, p=[epsilon,1-epsilon]))\n",
    "    \n",
    "    # pick the best action\n",
    "    best_defender_action_index = np.argmax(defender_policy)\n",
    "    best_invader_action_index = np.argmax(invader_policy)\n",
    "    \n",
    "    if action_type == 0:\n",
    "        \n",
    "        # randomly pick an action\n",
    "        random_defender_action_index = random.choice(range(4))    \n",
    "        random_invader_action_index = random.choice(range(4))    \n",
    "\n",
    "        # while random action is the same as the best action, pick a new action\n",
    "        while random_defender_action_index == best_defender_action_index:\n",
    "            random_defender_action_index = random.choice(range(4))\n",
    "        defender_action_index = random_defender_action_index\n",
    "        \n",
    "        # while random action is the same as the best action, pick a new action\n",
    "        while random_invader_action_index == best_invader_action_index:\n",
    "            random_invader_action_index = random.choice(range(4))\n",
    "        invader_action_index = random_invader_action_index\n",
    "    \n",
    "    else:\n",
    "        defender_action_index = best_defender_action_index\n",
    "        invader_action_index = best_invader_action_index\n",
    "    \n",
    "    joint_action = [defender_action_index, invader_action_index]\n",
    "    \n",
    "    return joint_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate initial states randomly\n",
    "def generate_random_initial_states(a):\n",
    "    random_initial_states = []\n",
    "    for i in range(a):\n",
    "        random_number = random.randint(0,5)\n",
    "        random_initial_states.append(random_number)\n",
    "    return tuple(random_initial_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimax Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize params\n",
    "t = 0\n",
    "T = 200\n",
    "lr = 0.9\n",
    "gamma = 0.95\n",
    "epsilon = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Q matrix\n",
    "state_action_pair_list = []\n",
    "\n",
    "# create every possible state action pairs: \n",
    "# 1296 states * 4 defender actions * 4 invader actions = 20736 s,a pairs\n",
    "for state in invader_defender.game_state_list:\n",
    "    for defender_action in range(action_count):\n",
    "        for invader_action in range(action_count):\n",
    "            joint_action = [defender_action, invader_action]\n",
    "            state_action_pair = list(state) + joint_action\n",
    "            state_action_pair_list.append(tuple(state_action_pair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a dictionary for Q values = {(x1, y1, x2, y2, defender_action_index, invader_action_index): q_value}\n",
    "listofzeros = [0.0] * len(state_action_pair_list)\n",
    "Q = dict(zip(state_action_pair_list, listofzeros))\n",
    "\n",
    "# initialize a dictionary for G values = {(x1, y1, x2, y2): payoff_matrix}\n",
    "listofzeros = [0.0] * len(invader_defender.game_state_list)\n",
    "for state in invader_defender.game_state_list:\n",
    "    state_list.append(state)\n",
    "G = dict(zip(state_list, listofzeros))\n",
    "\n",
    "# initialize policies\n",
    "initial_policy = []\n",
    "defender_policy = {}\n",
    "invader_policy = {}\n",
    "for i in range(len(invader_defender.game_state_list)):\n",
    "    random_policy = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "    initial_policy.append(random_policy)\n",
    "defender_policy = dict(zip(state_list, initial_policy))\n",
    "invader_policy = dict(zip(state_list, initial_policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Starts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize states\n",
    "defender_state = [5,0]\n",
    "invader_state = [0,0]\n",
    "current_state = tuple(defender_state + invader_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build game based on Q value\n",
    "G[current_state] = calculate_payoff(current_state)\n",
    "G[current_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a policy by solving the current game\n",
    "defender_policy[current_state], invader_policy[current_state] = equilibrium(G[current_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 0., 0., 0.]), array([1., 0., 0., 0.]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equilibrium(G[current_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-dfe3e1ebe4ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# print k and current max delta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'episode: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' timestep: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' unique Q: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# display all Q entries with non zero values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0;31m# kwarg-specified metadata gets precedence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0m_merge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmd_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmd_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdisplay_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDisplayHandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mpublish_display_data\u001b[0;34m(data, metadata, source, transient, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     )\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/ipykernel/zmqshell.py\u001b[0m in \u001b[0;36mpublish\u001b[0;34m(self, data, metadata, source, transient, update)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         self.session.send(\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         )\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, stream, msg_or_type, content, parent, ident, buffers, track, header, metadata)\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt_version\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         \u001b[0mto_send\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m         \u001b[0mto_send\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0mlongest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mto_send\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mserialize\u001b[0;34m(self, msg, ident)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m         real_message = [self.pack(msg['header']),\n\u001b[0;32m--> 636\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parent_header'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metadata'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m                         \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;31m# disallow nan, because it's not actually valid JSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m json_packer = lambda obj: jsonapi.dumps(obj, default=date_default,\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m )\n\u001b[1;32m    105\u001b[0m \u001b[0mjson_unpacker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjsonapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/zmq/utils/jsonapi.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(o, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'separators'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsonmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mcheck_circular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_circular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_nan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mseparators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseparators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         **kw).encode(obj)\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/jupyter_client/jsonutil.py\u001b[0m in \u001b[0;36mdate_default\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_tzinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'+00:00'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Z'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%r is not JSON serializable\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# trajectory = []\n",
    "# episodes = 20000\n",
    "eps = 0\n",
    "unique_counter = 0\n",
    "# for eps in range(episodes):\n",
    "    \n",
    "while unique_counter <= 20736:\n",
    "    eps +=1\n",
    "    t = 0\n",
    "    while t < T:\n",
    "\n",
    "        # keep track of trajectory\n",
    "#         trajectory.append(current_state)\n",
    "\n",
    "        # choose a joint based on epsilon greedy (joint_action = [a1_indx, a2_indx])\n",
    "        joint_action = choose_action(defender_policy[current_state], invader_policy[current_state], epsilon)\n",
    "        current_state_action_pair = tuple(list(current_state) + joint_action) # ie. (x1, y1, x2, y2, a1_indx, a2_indx)\n",
    "\n",
    "        # get next state and reward based on current state [x1,y1,x2,y2] and joint action [a1_indx, a2_indx]\n",
    "        next_state, reward = invader_defender.next_state(current_state, actions[joint_action[0]], actions[joint_action[1]])\n",
    "        next_state = tuple(next_state)\n",
    "        \n",
    "        # if game reached terminal state, restart new episode with random initial states\n",
    "        terminal, status = invader_defender.terminal_check(list(next_state))\n",
    "        if terminal:\n",
    "            current_state = generate_random_initial_states(4)\n",
    "            break\n",
    "        \n",
    "        # build a game based on next state: calculate payoff of next state\n",
    "        G[next_state] = calculate_payoff(next_state)\n",
    "\n",
    "        # generate a policy based on equilibirum of next game\n",
    "        defender_policy[next_state], invader_policy[next_state] = equilibrium(G[next_state])\n",
    "\n",
    "        # calculate the value of the next game\n",
    "        value = calculate_value(G[next_state])\n",
    "\n",
    "        # update Q[s,a] <- Q[s,a] + lr*(reward + gamma*value(s') - Q[s,a])\n",
    "        Q[current_state_action_pair] = Q[current_state_action_pair] + lr*(reward + gamma*value - Q[current_state_action_pair])\n",
    "\n",
    "        # set next state as current state\n",
    "        current_state = next_state\n",
    "        t+=1\n",
    "\n",
    "        # print k and current max delta\n",
    "        clear_output(wait=True)\n",
    "        display('episode: ' + str(eps) + ' timestep: ' + str(t) + ' unique Q: ' + str(unique_counter))\n",
    "    \n",
    "    # display all Q entries with non zero values\n",
    "    unique_counter = 0\n",
    "    for key in Q:\n",
    "        if Q[key] != 0.0:\n",
    "            unique_counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animate the trajectory\n",
    "game_trajectory = trajectory\n",
    "\n",
    "# define game dimensions\n",
    "columns=range(invader_defender.size)\n",
    "index = range(invader_defender.size)\n",
    "\n",
    "# animate the game\n",
    "for step in range(len(game_trajectory)):\n",
    "    game_table = pd.DataFrame(0, index = index, columns=columns)\n",
    "    game_table[4][4] = 'Ter.'\n",
    "    game_table[game_trajectory[step][1]][game_trajectory[step][0]] = 'DEF'\n",
    "    game_table[game_trajectory[step][3]][game_trajectory[step][2]] = 'INV'\n",
    "    clear_output(wait=True)\n",
    "    display(game_table, step)\n",
    "    time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all Q entries with non zero values\n",
    "unique_counter = 0\n",
    "\n",
    "for key in test_Q:\n",
    "    if test_Q[key] != 0.0:\n",
    "#         print(key, Q[key])\n",
    "        unique_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10936\n"
     ]
    }
   ],
   "source": [
    "# stopped running at 10936 unique Qs\n",
    "print(unique_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Q.pickle', 'wb') as handle:\n",
    "    pickle.dump(Q, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('Q.pickle', 'rb') as handle:\n",
    "    test_Q = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy extraction\n",
    "for state in invader_defender.game_state_list:    \n",
    "    G[state] = calculate_payoff(state)\n",
    "    defender_policy[state], invader_policy[state] = equilibrium(G[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defender_policy[(5,0,0,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invader_policy[(5,0,0,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defender_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defender_policy[(2,1,4,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invader_policy[(2,1,4,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results to Pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # takes ~ 50 minutes (132 iterations) to converge to within tolerance, \n",
    "# # so I am saving the learned U and G as a pickle\n",
    "# # to load them up faster (for development purpose)\n",
    "\n",
    "# with open('U.pickle', 'wb') as handle:\n",
    "#     pickle.dump(U, handle)\n",
    "\n",
    "# with open('G.pickle', 'wb') as handle:\n",
    "#     pickle.dump(G, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment to load U and G\n",
    "\n",
    "# with open ('U.pickle', 'rb') as handle:\n",
    "#     U = pickle.load(handle)\n",
    "    \n",
    "# with open ('G.pickle', 'rb') as handle:\n",
    "#     G = pickle.load(handle)\n",
    "\n",
    "# # converged k. This gives the last update to U dict\n",
    "# k = 132"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize policies\n",
    "# defender_policy = {}\n",
    "# invader_policy = {}\n",
    "# state_counter = 0\n",
    "\n",
    "# # policy extraction\n",
    "# for state in invader_defender.game_state_list:    \n",
    "#     G[state] = calculate_payoff(state)\n",
    "#     defender_policy[state], invader_policy[state] = equilibrium(G[state])\n",
    "#     state_counter += 1\n",
    "#     clear_output(wait=True)\n",
    "#     display('State: ' + str(state_counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a list of states that fixes the defender's starting position\n",
    "# fixed_defender_state_list = []\n",
    "# for invader_state in invader_defender.states:\n",
    "#     fixed_defender_state = [5, 0] + invader_state\n",
    "#     fixed_defender_state_list.append(fixed_defender_state)\n",
    "\n",
    "# # create invader heatmap\n",
    "# invader_map = np.zeros([6,6])\n",
    "# for state in fixed_defender_state_list:\n",
    "#     invader_map[state[2], state[3]] = U[k][tuple(state)]*-1 # -1 for invaders perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if the defender is fixed at the bottom left corner, this heatmap shows the invader's rewards\n",
    "# plt.imshow(invader_map, interpolation='nearest')\n",
    "# plt.colorbar()\n",
    "# plt.title('Value Function from the Invader Perspective (Defender fixed at [5,0])')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a list of states that fixes the invaders's starting position\n",
    "# fixed_invader_state_list = []\n",
    "# for defender_state in invader_defender.states:\n",
    "#     fixed_invader_state = defender_state + [0, 0]\n",
    "#     fixed_invader_state_list.append(fixed_invader_state)\n",
    "\n",
    "# # create invader heatmap\n",
    "# defender_map = np.zeros([6,6])\n",
    "# for state in fixed_invader_state_list:\n",
    "#     defender_map[state[0], state[1]] = U[k][tuple(state)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if invader is fixed at top left corner, this heatmap shows the defender's rewards\n",
    "# plt.imshow(defender_map, interpolation='nearest')\n",
    "# plt.colorbar()\n",
    "# plt.title('Value Function from the Defender Perspective (Invader fixed at [0,0])')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Delta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(delta_list)\n",
    "# plt.title('Iteration vs Delta')\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.xticks(np.arange(0, k, k/10))\n",
    "# plt.ylabel('Delta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play a Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_trajectory(Defender_state, Invader_state):\n",
    "#     game_trajectory = []\n",
    "#     terminal = False\n",
    "#     current_state = tuple(Defender_state + Invader_state)\n",
    "#     game_step = 0\n",
    "    \n",
    "#     # generate a game trajectory\n",
    "#     while not terminal:\n",
    "\n",
    "#         # append game trajectory\n",
    "#         game_trajectory.append(current_state)\n",
    "               \n",
    "#         # check if game is terminal (someone won)\n",
    "#         terminal, status = invader_defender.terminal_check(list(current_state))\n",
    "        \n",
    "#         # both agents choose action based on policy via sampling\n",
    "#         invader_action = actions[int(np.random.choice(action_count, 1, p=invader_policy[tuple(current_state)]))]\n",
    "#         defender_action = actions[int(np.random.choice(action_count, 1, p=defender_policy[tuple(current_state)]))]\n",
    "        \n",
    "#         # obtain next state\n",
    "#         next_state, reward = invader_defender.next_state(list(current_state), defender_action, invader_action)\n",
    "#         current_state = tuple(next_state)\n",
    "        \n",
    "#         game_step += 1\n",
    "#         clear_output(wait=True)\n",
    "#         display(\"game step: \" + str(game_step))\n",
    "        \n",
    "    \n",
    "#     return game_trajectory, status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animate the Game "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate game trajectory\n",
    "# game_trajectory, status = generate_trajectory([0,0],[0,5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define game dimensions\n",
    "# columns=range(invader_defender.size)\n",
    "# index = range(invader_defender.size)\n",
    "\n",
    "# # animate the game\n",
    "# for step in range(len(game_trajectory)):\n",
    "#     game_table = pd.DataFrame(0, index = index, columns=columns)\n",
    "#     game_table[4][4] = 'Ter.'\n",
    "#     game_table[game_trajectory[step][1]][game_trajectory[step][0]] = 'DEF'\n",
    "#     game_table[game_trajectory[step][3]][game_trajectory[step][2]] = 'INV'\n",
    "#     clear_output(wait=True)\n",
    "#     display(game_table)\n",
    "#     time.sleep(0.1)\n",
    "    \n",
    "# # print game status\n",
    "# display(status)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
